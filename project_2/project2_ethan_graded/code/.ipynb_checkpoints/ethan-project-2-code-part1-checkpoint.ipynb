{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Statement</a></span></li><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"#Import-Data-and-Overview\" data-toc-modified-id=\"Import-Data-and-Overview-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Import Data and Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-data-dictionary\" data-toc-modified-id=\"Read-data-dictionary-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Read data dictionary</a></span></li><li><span><a href=\"#Look-at-heads-and-tails-of-the-two-data-sets\" data-toc-modified-id=\"Look-at-heads-and-tails-of-the-two-data-sets-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Look at heads and tails of the two data sets</a></span></li><li><span><a href=\"#&quot;Forced&quot;-description-of-data-sets\" data-toc-modified-id=\"&quot;Forced&quot;-description-of-data-sets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>\"Forced\" description of data sets</a></span></li><li><span><a href=\"#Initial-appreciation-of-data,-before-cleaning-starts\" data-toc-modified-id=\"Initial-appreciation-of-data,-before-cleaning-starts-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Initial appreciation of data, before cleaning starts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-pair-plots\" data-toc-modified-id=\"Using-pair-plots-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Using pair plots</a></span></li><li><span><a href=\"#Using-correlation-heatmap\" data-toc-modified-id=\"Using-correlation-heatmap-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Using correlation heatmap</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-strategy\" data-toc-modified-id=\"Cleaning-strategy-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Cleaning strategy</a></span></li><li><span><a href=\"#Convert-column-names\" data-toc-modified-id=\"Convert-column-names-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Convert column names</a></span></li><li><span><a href=\"#Check-dtypes-and-convert-if-necessary\" data-toc-modified-id=\"Check-dtypes-and-convert-if-necessary-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Check dtypes and convert if necessary</a></span></li><li><span><a href=\"#Check-for-null-values\" data-toc-modified-id=\"Check-for-null-values-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Check for null values</a></span></li><li><span><a href=\"#Fix-null-values-(and-1st-round-of-feature-engineering*-if-suitable)\" data-toc-modified-id=\"Fix-null-values-(and-1st-round-of-feature-engineering*-if-suitable)-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Fix null values (and 1st round of feature engineering* if suitable)</a></span><ul class=\"toc-item\"><li><span><a href=\"#pool_qc\" data-toc-modified-id=\"pool_qc-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>pool_qc</a></span></li><li><span><a href=\"#Create-tracking-table-for-converted-ordinal-variables\" data-toc-modified-id=\"Create-tracking-table-for-converted-ordinal-variables-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Create tracking table for converted ordinal variables</a></span></li><li><span><a href=\"#misc_feature\" data-toc-modified-id=\"misc_feature-4.5.3\"><span class=\"toc-item-num\">4.5.3&nbsp;&nbsp;</span>misc_feature</a></span></li><li><span><a href=\"#alley\" data-toc-modified-id=\"alley-4.5.4\"><span class=\"toc-item-num\">4.5.4&nbsp;&nbsp;</span>alley</a></span></li><li><span><a href=\"#fence\" data-toc-modified-id=\"fence-4.5.5\"><span class=\"toc-item-num\">4.5.5&nbsp;&nbsp;</span>fence</a></span></li><li><span><a href=\"#fireplace_qu\" data-toc-modified-id=\"fireplace_qu-4.5.6\"><span class=\"toc-item-num\">4.5.6&nbsp;&nbsp;</span>fireplace_qu</a></span></li><li><span><a href=\"#lot_frontage\" data-toc-modified-id=\"lot_frontage-4.5.7\"><span class=\"toc-item-num\">4.5.7&nbsp;&nbsp;</span>lot_frontage</a></span></li><li><span><a href=\"#garage_finish,-garage_qual----,-garage_yr_blt,-garage_cond,-garage_type\" data-toc-modified-id=\"garage_finish,-garage_qual----,-garage_yr_blt,-garage_cond,-garage_type-4.5.8\"><span class=\"toc-item-num\">4.5.8&nbsp;&nbsp;</span>garage_finish, garage_qual    , garage_yr_blt, garage_cond, garage_type</a></span></li><li><span><a href=\"#Convert-garage_finish,-garage_qual,-garage_cond-to-numerical-ordinal-values\" data-toc-modified-id=\"Convert-garage_finish,-garage_qual,-garage_cond-to-numerical-ordinal-values-4.5.9\"><span class=\"toc-item-num\">4.5.9&nbsp;&nbsp;</span>Convert garage_finish, garage_qual, garage_cond to numerical ordinal values</a></span></li><li><span><a href=\"#bsmt_exposure,-bsmtfin_type_2,-bsmtfin_type_1,-bsmt_cond,-bsmt_qual\" data-toc-modified-id=\"bsmt_exposure,-bsmtfin_type_2,-bsmtfin_type_1,-bsmt_cond,-bsmt_qual-4.5.10\"><span class=\"toc-item-num\">4.5.10&nbsp;&nbsp;</span>bsmt_exposure, bsmtfin_type_2, bsmtfin_type_1, bsmt_cond, bsmt_qual</a></span></li><li><span><a href=\"#mas_vnr_area,-mas_vnr_type\" data-toc-modified-id=\"mas_vnr_area,-mas_vnr_type-4.5.11\"><span class=\"toc-item-num\">4.5.11&nbsp;&nbsp;</span>mas_vnr_area, mas_vnr_type</a></span></li><li><span><a href=\"#Others\" data-toc-modified-id=\"Others-4.5.12\"><span class=\"toc-item-num\">4.5.12&nbsp;&nbsp;</span>Others</a></span></li><li><span><a href=\"#Final-check-for-null-values\" data-toc-modified-id=\"Final-check-for-null-values-4.5.13\"><span class=\"toc-item-num\">4.5.13&nbsp;&nbsp;</span>Final check for null values</a></span></li></ul></li></ul></li><li><span><a href=\"#EDA-(and-2nd-round-of-feature-engineering*)\" data-toc-modified-id=\"EDA-(and-2nd-round-of-feature-engineering*)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>EDA (and 2nd round of feature engineering*)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-variables\" data-toc-modified-id=\"Categorical-variables-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Categorical variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-box-plots-of-all-categoricals-first\" data-toc-modified-id=\"Plot-box-plots-of-all-categoricals-first-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Plot box plots of all categoricals first</a></span></li><li><span><a href=\"#Create-fresh-df-to-start-storing-chosen-variables\" data-toc-modified-id=\"Create-fresh-df-to-start-storing-chosen-variables-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Create fresh df to start storing chosen variables</a></span></li><li><span><a href=\"#street\" data-toc-modified-id=\"street-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>street</a></span></li><li><span><a href=\"#neighborhood\" data-toc-modified-id=\"neighborhood-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>neighborhood</a></span></li><li><span><a href=\"#condition_1\" data-toc-modified-id=\"condition_1-5.1.5\"><span class=\"toc-item-num\">5.1.5&nbsp;&nbsp;</span>condition_1</a></span></li><li><span><a href=\"#condition_2\" data-toc-modified-id=\"condition_2-5.1.6\"><span class=\"toc-item-num\">5.1.6&nbsp;&nbsp;</span>condition_2</a></span></li><li><span><a href=\"#overall_qual\" data-toc-modified-id=\"overall_qual-5.1.7\"><span class=\"toc-item-num\">5.1.7&nbsp;&nbsp;</span>overall_qual</a></span></li><li><span><a href=\"#mas_vnr_type\" data-toc-modified-id=\"mas_vnr_type-5.1.8\"><span class=\"toc-item-num\">5.1.8&nbsp;&nbsp;</span>mas_vnr_type</a></span></li><li><span><a href=\"#exter_qual\" data-toc-modified-id=\"exter_qual-5.1.9\"><span class=\"toc-item-num\">5.1.9&nbsp;&nbsp;</span>exter_qual</a></span></li><li><span><a href=\"#exter_cond\" data-toc-modified-id=\"exter_cond-5.1.10\"><span class=\"toc-item-num\">5.1.10&nbsp;&nbsp;</span>exter_cond</a></span></li><li><span><a href=\"#foundation\" data-toc-modified-id=\"foundation-5.1.11\"><span class=\"toc-item-num\">5.1.11&nbsp;&nbsp;</span>foundation</a></span></li><li><span><a href=\"#bsmt_qual\" data-toc-modified-id=\"bsmt_qual-5.1.12\"><span class=\"toc-item-num\">5.1.12&nbsp;&nbsp;</span>bsmt_qual</a></span></li><li><span><a href=\"#bsmt_exposure\" data-toc-modified-id=\"bsmt_exposure-5.1.13\"><span class=\"toc-item-num\">5.1.13&nbsp;&nbsp;</span>bsmt_exposure</a></span></li><li><span><a href=\"#bsmtfin_type_1\" data-toc-modified-id=\"bsmtfin_type_1-5.1.14\"><span class=\"toc-item-num\">5.1.14&nbsp;&nbsp;</span>bsmtfin_type_1</a></span></li><li><span><a href=\"#bsmtfin_type_2\" data-toc-modified-id=\"bsmtfin_type_2-5.1.15\"><span class=\"toc-item-num\">5.1.15&nbsp;&nbsp;</span>bsmtfin_type_2</a></span></li><li><span><a href=\"#heating\" data-toc-modified-id=\"heating-5.1.16\"><span class=\"toc-item-num\">5.1.16&nbsp;&nbsp;</span>heating</a></span></li><li><span><a href=\"#heating_qc\" data-toc-modified-id=\"heating_qc-5.1.17\"><span class=\"toc-item-num\">5.1.17&nbsp;&nbsp;</span>heating_qc</a></span></li><li><span><a href=\"#central_air\" data-toc-modified-id=\"central_air-5.1.18\"><span class=\"toc-item-num\">5.1.18&nbsp;&nbsp;</span>central_air</a></span></li><li><span><a href=\"#electrical\" data-toc-modified-id=\"electrical-5.1.19\"><span class=\"toc-item-num\">5.1.19&nbsp;&nbsp;</span>electrical</a></span></li><li><span><a href=\"#kitchen_qual\" data-toc-modified-id=\"kitchen_qual-5.1.20\"><span class=\"toc-item-num\">5.1.20&nbsp;&nbsp;</span>kitchen_qual</a></span></li><li><span><a href=\"#functional\" data-toc-modified-id=\"functional-5.1.21\"><span class=\"toc-item-num\">5.1.21&nbsp;&nbsp;</span>functional</a></span></li><li><span><a href=\"#fireplace_qu\" data-toc-modified-id=\"fireplace_qu-5.1.22\"><span class=\"toc-item-num\">5.1.22&nbsp;&nbsp;</span>fireplace_qu</a></span></li><li><span><a href=\"#garage_finish\" data-toc-modified-id=\"garage_finish-5.1.23\"><span class=\"toc-item-num\">5.1.23&nbsp;&nbsp;</span>garage_finish</a></span></li><li><span><a href=\"#garage_qual\" data-toc-modified-id=\"garage_qual-5.1.24\"><span class=\"toc-item-num\">5.1.24&nbsp;&nbsp;</span>garage_qual</a></span></li><li><span><a href=\"#garage_cond\" data-toc-modified-id=\"garage_cond-5.1.25\"><span class=\"toc-item-num\">5.1.25&nbsp;&nbsp;</span>garage_cond</a></span></li><li><span><a href=\"#paved_drive\" data-toc-modified-id=\"paved_drive-5.1.26\"><span class=\"toc-item-num\">5.1.26&nbsp;&nbsp;</span>paved_drive</a></span></li><li><span><a href=\"#sale_type\" data-toc-modified-id=\"sale_type-5.1.27\"><span class=\"toc-item-num\">5.1.27&nbsp;&nbsp;</span>sale_type</a></span></li><li><span><a href=\"#Selected-and-transformed-categorical-features\" data-toc-modified-id=\"Selected-and-transformed-categorical-features-5.1.28\"><span class=\"toc-item-num\">5.1.28&nbsp;&nbsp;</span>Selected and transformed categorical features</a></span></li></ul></li><li><span><a href=\"#Numerical-variables\" data-toc-modified-id=\"Numerical-variables-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Numerical variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-outliers\" data-toc-modified-id=\"Check-for-outliers-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Check for outliers</a></span></li><li><span><a href=\"#Choose-outliers-to-remove\" data-toc-modified-id=\"Choose-outliers-to-remove-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Choose outliers to remove</a></span></li></ul></li><li><span><a href=\"#Correlation-heatmap-(and-3rd-round-of-feature-engineering)\" data-toc-modified-id=\"Correlation-heatmap-(and-3rd-round-of-feature-engineering)-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Correlation heatmap (and 3rd round of feature engineering)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Low-correlation-with-prices\" data-toc-modified-id=\"Low-correlation-with-prices-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Low correlation with prices</a></span></li><li><span><a href=\"#Multicollinearity\" data-toc-modified-id=\"Multicollinearity-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Multicollinearity</a></span></li></ul></li></ul></li><li><span><a href=\"#Summary-of-final-datasets\" data-toc-modified-id=\"Summary-of-final-datasets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary of final datasets</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames Housing Project by Ethan Leow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this notebook was created with the help of Nbextensions' \"Table of Contents (2)\" and is best viewed / navigated by enabling the extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "I work for a real estate consultancy in Ames, IA. My boss has tasked me to come up with a good predictive model for house prices in my city. The main purpose is to help our clients, i.e. buyers of homes, shortlist undervalued houses and understand features of a property that are important drivers of sale prices. To do so, I need to come up with a predictive model that can generate an expected value for a property given all its attributes, and the client can compare it with the asking price of the seller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Lasso,\n",
    "    LassoCV,\n",
    "    Ridge,\n",
    "    RidgeCV,\n",
    "    ElasticNet,\n",
    "    ElasticNetCV\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_squared_error\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train = pd.read_csv(\"../datasets/train.csv\")\n",
    "test = pd.read_csv(\"../datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at data dictionary (source: Dean De Cock, Truman State University):\n",
    "__[data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)__\n",
    "I shall be editing this dictionary to make it more concise and relevant after data cleaning is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at heads and tails of the two data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at heads and tails of data sets\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train has 2051 rows and 81 columns, while Test has 878 and 80 columns. \n",
    "In line with expectations, as the Test set is supposed to have one less column than Train set.\n",
    "Confirmed when I scroll to the right, that the \"SalePrice\" column is missing for Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the [data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt) mentions 82 columns (or variables), I can't help but notice that one variable is missing in both Train and Test sets as they have 81 and 80 columns respectively. I looked through and found that the missing variable is Sale Condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Forced\" description of data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all columns and rows to max for describe() dataset\n",
    "pd.set_option('max_columns',None)\n",
    "pd.set_option('max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe train and test set, including categorical and nominal variables\n",
    "display(train.describe(include = 'all'))\n",
    "display(test.describe(include = 'all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initial_appreciation_of_data'></a>\n",
    "### Initial appreciation of data, before cleaning starts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pair plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cut look of numeric variable, using sns.pairplot()\n",
    "# DO NOT RUN THE BELOW COMMAND UNLESS YOU HAVE TIME, WILL TAKE MORE THAN 30 MINS!!\n",
    "\n",
    "# sns.pairplot(train.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# A .png file has been saved in ../images for us to review at our leisure anytime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pairplot of all numeric variables in Train set - First Cut](../images/train_numeric_pairplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot of \"Sale Price\" against all continuous variables, and categorical variables that are encoded as numbers\n",
    "    - We start to see some rough patterns...and potential outliers to fix during data cleaning later\n",
    "    - Things to mentally take note of:\n",
    "        - Lot frontage: Slightly upward trend where larger lot frontage equates to higher prices. 2 potential outliers where lot frontage are extremely large but prices are low\n",
    "        - Lot area: Same issue with 2 possible outliers where transacted prices look too low for the lot sizes\n",
    "        - Overall qual: Definitely ordinal with 10 discrete vertical lines. Must keep this variable, as it looks well behaved with higher sale prices for higher quality.\n",
    "        - Overall cond: Weird vertical line at value of \"5\", prices stretch from the min to max. Might be the case that people just put \"5\" as a default regardless of how good the house is. All other values (1-4, 6-10) look fine, with an upward sloping relationship to price.\n",
    "        - Year built: Can keep, upward sloping relationship\n",
    "        - Year remodeled: Upward sloping relationship, can possibly keep this variable if not too correlated with year_built\n",
    "        - Mas Vnr Area: Nothing interesting, no obvious relationship\n",
    "        - BsmtFin SF 1: Two outliers (>4000 sqft) with no obvious increase in price even though size is so large. \n",
    "        - BsmtFin SF2, Bsmt Unf SF: No obvious relationship, may drop later if its data is not clean\n",
    "        - Total Bsmt SF: Can keep, strong upward sloping relationship. Must fix two outliers (~5000 sqft and ~6000 sqft) that did not show correspondingly large sales prices\n",
    "        - 1st Flr SF: Can keep, strong upward sloping relationship. 2-3 potential outliers to fix.\n",
    "        - 2nd Flr SF: Interesting data series. I see the full range of sale prices (min to max) clustered at the SF=0 value (i.e. vertical line), and an upward sloping relationship between price and SF for SF >100. We need to transform this variable, otherwise the regression coefficient will be messed up by all the sale prices at SF=0. Perhaps create dummy + interaction variables during the cleaning stage later, or simply add 1st Flr SF to 2nd Flr SF?\n",
    "        - Low Qual Fin SF: Can drop later, no obvious relationship\n",
    "        - Gr Liv Area: Good variable to keep, need to fix two outliers for area > 5000sqft. To check on correlation with 1st_Flr_SF and 2nd_Flr_SF later, if it is high, maybe we don't need the latter two variables for our regression.\n",
    "        - Bsmt Full Bath, Half Bath: Nothing interesting here.\n",
    "        - Full Bath: Can possibly keep, upward sloping relationship\n",
    "        - Bedroom AbvGr, Kitchen AbvGr: Nothing obvious here\n",
    "        - TotRms AbvGrd: Can keep, but 3 possible outliers (13-15 rooms) with less than average prices\n",
    "        - Fireplaces: Weakly upward sloping relationship\n",
    "        - GarageYr Blt: Upward sloping, but 1 obvious outlier / data error to correct (built around 2200!)\n",
    "        - Garage Cars / Garage Area: Both upward sloping, can keep, but need to see corr matrix below to decide if it's better to drop one of them to avoid collinearity issues\n",
    "        - Wood Deck SF, Open Porch SF: No obvious relationship\n",
    "        - Enclosed Porch, 3Ssn Porch, Screen Porch, Pool Area: No obvious relationship\n",
    "        - Misc Val: No obvious relationship\n",
    "        - Month Sold: No obvious relationship\n",
    "        - Year Sold: No obvious relationship, but probably better to keep it in to account for possible time trend, or dummify it to account for 2008-2010's recession / foreclosures??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap to substantiate prelim findings\n",
    "# Like the pairplot above, this is a very rough first cut, as we have not cleaned the data yet,\n",
    "# and we have not converted or transformed nominal and \n",
    "corr = train.select_dtypes(include=['float64', 'int64']).corr()\n",
    "\n",
    "plt.figure(figsize=(20,15)) \n",
    "\n",
    "sns.heatmap(\n",
    "    data = corr, \n",
    "#    annot = True,\n",
    "    linewidths = 3,\n",
    "    cmap = \"coolwarm\",\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation heat map confirms some of the observations seen in sns.pairplot. A few things to note during cleaning and EDA later:\n",
    "- Garage Cars and Garage Area have extremely high correlation, so we can drop one during data cleaning, whichever has lower quality data (e.g. more NAs)\n",
    "- TotalRms AveGrd and Gr Liv Area have very high correlation, but pairplot shows Gr Liv Area having tighter (less dispersed) datapoints than TotalRms when plotted against Sales Price, maybe it is better to drop TotalRms.\n",
    "- Garage Yr Blt and Year Blt have very high correlation, and strong relationship against sale price in the pair plots. Makes sense since most houses and their garages should be built at the same time, so we should probably drop Garage Yr Blt to avoid collinearity issues if it has more Null values than Year Blt (to double confirm during data cleaning stage)\n",
    "- Total Bsmt SF and 1st Flr SF have high correlation, can consider dropping 1st Flr SF as both show upward sloping relationship with Sale Price. 1st Flr SF is also correlated with Gr Liv Area, but Gr Liv Area is not as correlated with Total Bsmt SF, so we are not losing valuable info by dropping 1st Flr SF and retaining both Total Bsmt SF and Gr Liv Area.\n",
    "- In general, as a first cut, we can see the following variables being strong indicators of sale prices:\n",
    "    - Overall Qual\n",
    "    - Year Built\n",
    "    - Year Remod/Add\n",
    "    - Mas Vnr Area\n",
    "    - Total Bsmt SF\n",
    "    - Gr Liv Area\n",
    "    - Full Bath\n",
    "    - Garage Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning strategy\n",
    "\n",
    "My first step is to use deductive imputation to see if we can use logic to fill in null values. This requires reading the [data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt) provided by Dean De Cock.\n",
    "\n",
    "If deductive imputation does not work, instead of jumping to mean/median/mode imputation, I shall look at the results of my first-cut look of pair plots and correlation heatmap in the [initial appreciation of data](#initial_appreciation_of_data) section above. If a variable with null values is highly correlated with another variable that does not, we can drop the troublesome variable. If a variable has a lot of null values and does not seem to exhibit any relationship with sale price in the pair plot, we can also drop it. The reason is that mean/median/mode imputation can significantly distort the histogram and underestimate variance of the variable, and should be used only if we are reasonably confident data is MCAR. \n",
    "\n",
    "Finally, I shall also consider using regression imputation if there are other variables that are fully observed, only if it reduces the MSE of my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns naming to lowercase and replace spaces with underscores\n",
    "train.columns = [x.lower().replace(' ','_') for x in train.columns]\n",
    "test.columns = [x.lower().replace(' ','_') for x in test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "train.columns, test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dtypes and convert if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dtypes by count\n",
    "print(\"train set \\n\", train.dtypes.value_counts())\n",
    "print(\"\")\n",
    "print(\"test set \\n\", test.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to float, int64 to datetime, if necessary\n",
    "\n",
    "# Check columns to see if strings should be converted to float/int\n",
    "train.select_dtypes(include=['object']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good above, none of the columns in train set are not numerical values that are accidentally read in as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat exercise for test set\n",
    "test.select_dtypes(include=['object']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good above, none of the columns in test set are not numerical values that are accidentally read in as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if float64 or int64 need to be converted to datetime\n",
    "train.select_dtypes(include=['float', 'int']).columns, test.select_dtypes(include=['float', 'int']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the column names above seem to indicate a need to convert from float/int to datetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for number of missing values and percent missing\n",
    "train_null = pd.DataFrame(\n",
    "    {'null_count' : train.isnull().sum(),\n",
    "    'non_null_count' : train.count()\n",
    "    }\n",
    ")\n",
    "\n",
    "train_null[\"percent_missing\"] = train_null.null_count / \\\n",
    "                                (train_null.null_count + train_null.non_null_count)\n",
    "\n",
    "train_null.sort_values(\"percent_missing\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "test_null = pd.DataFrame(\n",
    "    {'null_count' : test.isnull().sum(),\n",
    "    'non_null_count' : test.count()\n",
    "    }\n",
    ")\n",
    "\n",
    "test_null[\"percent_missing\"] = test_null.null_count / \\\n",
    "                                (test_null.null_count + test_null.non_null_count)\n",
    "\n",
    "test_null.sort_values(\"percent_missing\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of findings\n",
    "- pool_qc, misc_feature, alley, fence and fireplace_qc are the biggest \"offenders\", with than 40% missing values\n",
    "- lot_frontage is next with 16-18% missing values\n",
    "- the remaining variables with missing data can be split into two categories (between 1-5% missing)\n",
    "    - Garage-related data, e.g. year built, finish, quality, condition, type\n",
    "    - Basement-related data, e.g. finish type, quality, condition, exposure\n",
    "    - Masonry veneer data, e.g. area and type\n",
    "- the rest are miscellaneous with one or two datapoints missing (<= 1% missing)\n",
    "\n",
    "I shall start my data cleaning with variables with the highest number of missing values: pool_qc, misc_feature..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix null values (and 1st round of feature engineering* if suitable)\n",
    "\\*to assign numerical values to categorial value where appropriate\n",
    "\n",
    "#### pool_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure \n",
    "print(\"Unique values are\", train.pool_qc.unique())\n",
    "print()\n",
    "print(\"Count of null values is\", np.sum(train.pool_qc.isnull()))\n",
    "print()\n",
    "print(\"Count of non-null values by type \\n\", train.pool_qc.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary says NA mean No Pool, so we can use deductive imputation here to set null values = \"none\".\n",
    "In addition, the remaining values (Fa, TA, Gd, Ex) have a logical order to them, in terms of increasing quality of pool construction. Therefore, I shall replace categorical values (none, Fa, TA, Gd, Ex) with numerical values (0-1-2-3-4) to reflect ordinal nature of pool quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary says NA mean No Pool, and the remaining values have an order in increasing quality\n",
    "# Replace with numerical values to reflect ordinal nature\n",
    "\n",
    "convert = {np.nan : 0,\n",
    "           'Fa' : 1,\n",
    "           'TA' : 2,\n",
    "           'Gd' : 3,\n",
    "           'Ex' : 4\n",
    "}\n",
    "\n",
    "train.pool_qc = train.pool_qc.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "train.pool_qc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.lmplot(data = train, x = \"pool_qc\", y = \"saleprice\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from string to number looks correct: higher pool_qc values seem to indicate higher sale prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "convert = {np.nan : 0,\n",
    "            'Fa' : 1,\n",
    "            'TA' : 2,\n",
    "            'Gd' : 3,\n",
    "            'Ex' : 4\n",
    "}\n",
    "\n",
    "test.pool_qc = test.pool_qc.map(convert)\n",
    "\n",
    "# Check \n",
    "test.pool_qc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tracking table for converted ordinal variables \n",
    "I shall create a table to help me keep track of which categorical variables i have transformed from strings to numerical. I expect it to be populated as I clean more categorical variables.\n",
    "\n",
    "| Variable | Original Categories (in order of quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NA -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### misc_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Unique values are\", train.misc_feature.unique())\n",
    "print()\n",
    "print(\"Count of null values is\", np.sum(train.misc_feature.isnull()))\n",
    "print()\n",
    "print(\"Count of non-null values by type \\n\", train.misc_feature.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary states these are miscellaneous feature not covered in other categories.\n",
    "\t\t\n",
    "       Elev\tElevator\n",
    "       Gar2\t2nd Garage (if not described in garage section)\n",
    "       Othr\tOther\n",
    "       Shed\tShed (over 100 SF)\n",
    "       TenC\tTennis Court\n",
    "       NA\tNone\n",
    "\n",
    "I shall convert null values to None. The remaining values do not have an order to them, so likely to use one hot encoding to convert them to dummies during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert null values to \"none\"\n",
    "train.misc_feature.fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "train.misc_feature.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.boxplot(data = train, x = \"misc_feature\", y = \"saleprice\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "test.misc_feature.fillna('None', inplace = True)\n",
    "\n",
    "# Check\n",
    "test.misc_feature.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Unique values are\", train.alley.unique())\n",
    "print()\n",
    "print(\"Count of null values is\", np.sum(train.alley.isnull()))\n",
    "print()\n",
    "print(\"Count of non-null values by type \\n\", train.alley.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of alley access to property\n",
    "\n",
    "       Grvl\tGravel\n",
    "       Pave\tPaved\n",
    "       NA \tNo alley access\n",
    "       \n",
    "Dictionary says NA means no access. I shall convert null values to None. The remaining values do not have an order to them, so likely to use one hot encoding to convert them to dummies during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert null values to \"none\"\n",
    "train.alley.fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "train.alley.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.boxplot(data = train, x = \"alley\", y = \"saleprice\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "test.alley.fillna('None', inplace = True)\n",
    "\n",
    "# Check\n",
    "test.alley.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Unique values are\", train.fence.unique())\n",
    "print()\n",
    "print(\"Count of null values is\", np.sum(train.fence.isnull()))\n",
    "print()\n",
    "print(\"Count of non-null values by type \\n\", train.fence.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fence (Ordinal): Fence quality\n",
    "\t\t\n",
    "       GdPrv\tGood Privacy\n",
    "       MnPrv\tMinimum Privacy\n",
    "       GdWo\t    Good Wood\n",
    "       MnWw\t    Minimum Wood/Wire\n",
    "       NA\t    No Fence\n",
    "       \n",
    "Dictionary says NA means no fence. I shall convert null values to None. The remaining values do really have an order to them (e.g. no relation between good wood vs good privacy), so likely to use one hot encoding to convert them to dummies during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert null values to \"none\"\n",
    "train.fence.fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "train.fence.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.boxplot(data = train, x = \"fence\", y = \"saleprice\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "test.fence.fillna('None', inplace = True)\n",
    "\n",
    "# Check\n",
    "test.fence.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fireplace_qu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Unique values are\", train.fireplace_qu.unique())\n",
    "print()\n",
    "print(\"Count of null values is\", np.sum(train.fireplace_qu.isnull()))\n",
    "print()\n",
    "print(\"Count of non-null values by type \\n\", train.fireplace_qu.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FireplaceQu (Ordinal): Fireplace quality\n",
    "\n",
    "       Ex\tExcellent - Exceptional Masonry Fireplace\n",
    "       Gd\tGood - Masonry Fireplace in main level\n",
    "       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n",
    "       Fa\tFair - Prefabricated Fireplace in basement\n",
    "       Po\tPoor - Ben Franklin Stove\n",
    "       NA\tNo Fireplace\n",
    "       \n",
    "Dictionary says NA means no fireplace. The remaining values have an order to them, so will convert to numerical (0-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NA and categorical values to numerical values\n",
    "convert = {np.nan : 0,\n",
    "            'Po' : 1,\n",
    "            'Fa' : 2,\n",
    "            'TA' : 3,\n",
    "            'Gd' : 4,\n",
    "            'Ex' : 5\n",
    "}\n",
    "\n",
    "train.fireplace_qu = train.fireplace_qu.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "train.fireplace_qu.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.lmplot(data = train, x = \"fireplace_qu\", y = \"saleprice\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from string to number looks correct: higher fireplace_qu values seem to indicate higher sale prices\n",
    "\n",
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NA -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NA -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "convert = {np.nan : 0,\n",
    "            'Po' : 1,\n",
    "            'Fa' : 2,\n",
    "            'TA' : 3,\n",
    "            'Gd' : 4,\n",
    "            'Ex' : 5\n",
    "}\n",
    "\n",
    "test.fireplace_qu = test.fireplace_qu.map(convert)\n",
    "\n",
    "# Check \n",
    "test.fireplace_qu.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lot_frontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Count of null values is\", np.sum(train.lot_frontage.isnull()))\n",
    "print()\n",
    "print(\"The structure of non-null values is \\n\", train.lot_frontage.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary say Lot Frontage = Linear feet of street connected to property.\n",
    "Is it possible that \"null\" means that the street is not connected to the property at all, or could it be MCAR?\n",
    "\n",
    "Filter to check the following variables when lot_frontage is null:\n",
    "- Street (Nominal): Type of road access to property\n",
    "       Grvl\tGravel\t\n",
    "       Pave\tPaved\n",
    "- Lot Config (Nominal): Lot configuration\n",
    "       Inside\tInside lot\n",
    "       Corner\tCorner lot\n",
    "       CulDSac\tCul-de-sac\n",
    "       FR2\tFrontage on 2 sides of property\n",
    "       FR3\tFrontage on 3 sides of property\n",
    "If the two variables are non-null when lot_frontage is null, it will be wrong to assign 0 or \"none\" to lot_frontage as the house obviously is exposed to the street. But if both street and lot_config are null when lot_frontage is null, then it really is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter data\n",
    "train.loc[train.lot_frontage.isnull(), [\"street\", \"lot_config\", \"lot_shape\", \"lot_area\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like null values are not zero values as all of them have road access and a number of them even have frontages on 2 or 3 sides of the property.\n",
    "I shall check if a regression imputation makes sense, as using mean/median/mode might skew the histogram too much. For example, a property with a large lot_area should have a larger lot_frontage, and I notice that there are zero missing values for lot_area, so that makes a good variable to create imputated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm lot_area has no missing values\n",
    "train.lot_area.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of lot_frontage against lot_area\n",
    "sns.lmplot(data = train, x = \"lot_area\", y = \"lot_frontage\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An imputation using basic regression looks workable, outliers don't seem to affect the slope much.\n",
    "I shall create dummy variables for \"neighborhood\" to further account for differences in location (e.g. properties located closer to the city centre might have small lot frontages, all else equal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up imputation regression\n",
    "\n",
    "# Pull in relevant variables\n",
    "df = train[['lot_frontage', 'lot_area', 'neighborhood']]\n",
    "\n",
    "# Create dummies for neighborhood variable\n",
    "df = pd.get_dummies(df, drop_first = True)\n",
    "\n",
    "y = df.loc[df.lot_frontage.notnull(), 'lot_frontage']\n",
    "X = df.loc[df.lot_frontage.notnull(), :].drop(columns = {\"lot_frontage\"})\n",
    "\n",
    "# Instantiate model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit model\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save indices of missing values\n",
    "missing_idx = train.index[train.lot_frontage.isnull()]\n",
    "missing_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the prediciton from the model\n",
    "df['lot_frontage_imputed'] = [\n",
    "    df.loc[\n",
    "        i,'lot_frontage'\n",
    "    ] if i not in missing_idx\n",
    "    else lr.predict(\n",
    "        pd.DataFrame(\n",
    "            df.loc[i,~df.columns.isin(['lot_frontage',\n",
    "                                       'lot_frontage_imputed'])]).T)[0]\n",
    "    for i in range(df.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An imputation using basic regression looks workable, outliers don't seem to affect the slope much.\n",
    "I shall create dummy variables for \"neighborhood\" to further account for differences in location (e.g. properties located closer to the city centre might have small lot frontages, all else equal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasonability check against mean and median imputation\n",
    "mean_frontage = df.lot_frontage.mean()\n",
    "median_frontage = df.lot_frontage.median()\n",
    "\n",
    "df['frontage_mean_imputed'] = train.lot_frontage.fillna(mean_frontage)\n",
    "df['frontage_median_imputed'] = train.lot_frontage.fillna(median_frontage)\n",
    "\n",
    "df[['lot_frontage','lot_area', 'lot_frontage_imputed',\n",
    "    'frontage_mean_imputed', 'frontage_median_imputed']\n",
    "  ].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like regression imputation is better than mean/median imputation, as the latter two seem to distort the histogram and artificially compress the variance of lot_frontage. Will go with regression imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge imputed values to train data set\n",
    "train = pd.merge(\n",
    "    left = train,\n",
    "    right = df['lot_frontage_imputed'],\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check merger is done correctly\n",
    "train.loc[:, ['lot_frontage', 'lot_frontage_imputed']].sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "train.lot_frontage_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(15,5))\n",
    "fig.suptitle('Plot of Sales Price against Lot Frontage and Imputed Lot Frontage')\n",
    "\n",
    "sns.regplot(data = train, x = \"lot_frontage\", y = \"saleprice\", ax = axes[0])\n",
    "sns.regplot(data = train, x = \"lot_frontage_imputed\", y = \"saleprice\", ax = axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation looks fine, the general slope is not affected. Need to investigate two large extrapolated values later during the EDA stage (one in the 300 ft and one in the 400 ft range).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "\n",
    "# Note to self: Do not run lr.fit() again, use the lr() with parameters fitted in train set\n",
    "\n",
    "df_test = test[['lot_frontage', 'lot_area', 'neighborhood']]\n",
    "\n",
    "# Create dummies for neighborhood variable\n",
    "df_test = pd.get_dummies(df_test, drop_first = True)\n",
    "\n",
    "# Check if neighborhood dummies for train set match those for test set\n",
    "set(train.neighborhood.unique()) - set(test.neighborhood.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This will create a problem when applying any regression that is fitted on train set to test set, as the test set will have two fewer dummy variables if i choose to use \"neighborhood\" in my final regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two additional dummies for test set so that regression \n",
    "# can be run on test set using train set fitted parameters\n",
    "df_test[['neighborhood_GrnHill', 'neighborhood_Landmrk']] = 0\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save indices of missing values\n",
    "missing_idx = test.index[test.lot_frontage.isnull()]\n",
    "missing_idx\n",
    "\n",
    "# Impute missing values with the prediciton from the model\n",
    "df_test['lot_frontage_imputed'] = [\n",
    "    df_test.loc[\n",
    "        i,'lot_frontage'\n",
    "    ] if i not in missing_idx\n",
    "    else lr.predict(\n",
    "        pd.DataFrame(\n",
    "            df_test.loc[i,~df_test.columns.isin(['lot_frontage',\n",
    "                                                 'lot_frontage_imputed'])]).T)[0]\n",
    "    for i in range(df_test.shape[0])\n",
    "]\n",
    "\n",
    "# Merge imputed values to test data set\n",
    "test = pd.merge(\n",
    "    left = test,\n",
    "    right = df_test['lot_frontage_imputed'],\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "test.lot_frontage_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2, that merger is done correctly\n",
    "test.loc[:, ['lot_frontage', 'lot_frontage_imputed']].sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### garage_finish, garage_qual\t, garage_yr_blt, garage_cond, garage_type\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I group these 5 variables together as they have the same number of missing values. It is quite likely that these are the same entries, so the same approach can be taken for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "to_convert = [\n",
    "        'garage_finish', \n",
    "        'garage_qual',\n",
    "#        'garage_yr_blt',\n",
    "        'garage_cond',\n",
    "        'garage_type'\n",
    "]\n",
    "\n",
    "for _ in to_convert:\n",
    "    print(\"Unique values for\", _, \"are\", train[_].unique())\n",
    "    print()\n",
    "    print(\"Count of null values is\", np.sum(train[_].isnull()))\n",
    "    print()\n",
    "    print(\"Count of non-null values by type \\n\", train[_].value_counts())\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all five variables are null together\n",
    "train.loc[\n",
    "    train.garage_finish.isnull(),\n",
    "    [\n",
    "        'garage_finish', \n",
    "        'garage_qual',\n",
    "        'garage_yr_blt',\n",
    "        'garage_cond',\n",
    "        'garage_type'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All are null except entry 1712, which looks to be an error. Dictionary states that NA means no garage, so it is not possible for index 1712 to have \"Detchd\" for garage_type when all 4 other entries for index 1712 state that the house does not have a garage. Will overwrite with None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NaN values to None for garage_finish, garage_qual , garage_yr_blt, garage_cond, garage_type\n",
    "to_convert = [\n",
    "        'garage_finish', \n",
    "        'garage_qual',\n",
    "        'garage_yr_blt',\n",
    "        'garage_cond',\n",
    "        'garage_type'\n",
    "]\n",
    "\n",
    "for _ in to_convert:\n",
    "    train[_].fillna('None', inplace = True)\n",
    "    \n",
    "# Convert index 1712's garage_type to \"None\"\n",
    "train.loc[1712, \"garage_type\"] = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "for _ in to_convert:\n",
    "    print(\"Count of null values for\", _, \"is\", np.sum(train[_].isnull()))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "\n",
    "# Check if all five variables are null together\n",
    "test.loc[\n",
    "    test.garage_finish.isnull(),\n",
    "    [\n",
    "        'garage_finish', \n",
    "        'garage_qual',\n",
    "        'garage_yr_blt',\n",
    "        'garage_cond',\n",
    "        'garage_type'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overwrite for index 764 as it is not possible to have a \"Detchd\" garage when all other 4 variables say that there is no garage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NaN values to None for garage_finish, garage_qual , garage_yr_blt, garage_cond, garage_type\n",
    "to_convert = [\n",
    "        'garage_finish', \n",
    "        'garage_qual',\n",
    "        'garage_yr_blt',\n",
    "        'garage_cond',\n",
    "        'garage_type'\n",
    "]\n",
    "\n",
    "for _ in to_convert:\n",
    "    test[_].fillna('None', inplace = True)\n",
    "    \n",
    "# Convert index 1712's garage_type to \"None\"\n",
    "test.loc[764, \"garage_type\"] = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "for _ in to_convert:\n",
    "    print(\"Count of null values for\", _, \"is\", np.sum(test[_].isnull()))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert garage_finish, garage_qual, garage_cond to numerical ordinal values\n",
    "Of the 5 \"garage_\" variables above, _finish, _qual and _cond are suitable to be converted to numerical values as they have a clear order in terms of increasing quality.\n",
    "\n",
    "Garage Finish (Ordinal)\t: Interior finish of the garage\n",
    "\n",
    "       Fin\tFinished\n",
    "       RFn\tRough Finished\t\n",
    "       Unf\tUnfinished\n",
    "       NA\tNo Garage\n",
    "       \n",
    "Garage Qual (Ordinal): Garage quality\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical/Average\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "       NA\tNo Garage\n",
    "\n",
    "Garage Cond (Ordinal): Garage condition\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical/Average\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "       NA\tNo Garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert _finish categorical values to numerical values for train and test sets\n",
    "convert = {'None' : 0,\n",
    "            'Unf' : 1,\n",
    "            'RFn' : 2,\n",
    "            'Fin' : 3,\n",
    "}\n",
    "\n",
    "train.garage_finish = train.garage_finish.map(convert)\n",
    "test.garage_finish = test.garage_finish.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert _qual and _cond categorical values to numerical values for train and test sets\n",
    "convert = {'None' : 0,\n",
    "            'Po' : 1,\n",
    "            'Fa' : 2,\n",
    "            'TA' : 3,\n",
    "            'Gd' : 4,\n",
    "            'Ex' : 5\n",
    "}\n",
    "\n",
    "train.garage_qual = train.garage_qual.map(convert)\n",
    "test.garage_qual = test.garage_qual.map(convert)\n",
    "train.garage_cond = train.garage_cond.map(convert)\n",
    "test.garage_cond = test.garage_cond.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "to_print = ['garage_finish', 'garage_qual', 'garage_cond']\n",
    "for _ in to_print:\n",
    "    print(\"Train set \\n\", train[_].value_counts())\n",
    "    print(\"Test set \\n\", test[_].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(15,5))\n",
    "fig.suptitle('Plot of Sales Price against Garage Finish, Cond & Qual')\n",
    "\n",
    "sns.regplot(data = train, x = \"garage_finish\", y = \"saleprice\", ax = axes[0])\n",
    "sns.regplot(data = train, x = \"garage_cond\", y = \"saleprice\", ax = axes[1])\n",
    "sns.regplot(data = train, x = \"garage_qual\", y = \"saleprice\", ax = axes[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from string to number looks correct: higher garage_finish, garage_cond and garage_qual values seem to indicate higher sale prices\n",
    "\n",
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \tbsmt_exposure, bsmtfin_type_2, bsmtfin_type_1, bsmt_cond, bsmt_qual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I group these 5 variables together as they have the same number of missing values. It is quite likely that these are the same entries, so the same approach can be taken for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "to_convert = [\n",
    "    'bsmt_exposure', \n",
    "    'bsmtfin_type_2', \n",
    "    'bsmtfin_type_1', \n",
    "    'bsmt_cond', \n",
    "    'bsmt_qual'\n",
    "]\n",
    "\n",
    "for _ in to_convert:\n",
    "    print(\"Unique values for\", _, \"are\", train[_].unique())\n",
    "    print()\n",
    "    print(\"Count of null values is\", np.sum(train[_].isnull()))\n",
    "    print()\n",
    "    print(\"Count of non-null values by type \\n\", train[_].value_counts())\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for entries that don't look right, i.e. one variable says no basement but another say basement exists\n",
    "\n",
    "# Use bsmt_exposure as reference point as it has the most number of null values\n",
    "\n",
    "train.loc[train.bsmt_exposure.isnull(), to_convert]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I note that there are 3 entries where bsmt_exposure == NaN but they have 3-4 other basement-related that are populated. All of them are Unf (unfinished). I shall find out what is the most common value assigned to bsmt_exposure for unfinished basements. Logically, it should be \"none\" as the basement is not ready yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train.bsmtfin_type_1 == \"Unf\", \"bsmt_exposure\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being the mode, I will input \"no\" to bsmt_exposure for basements that are unfinished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input bsmt_exposure value == \"no\" for entries 1456, 1547, 1997\n",
    "train.loc[[1456,1547,1997],\"bsmt_exposure\"] = \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if test has same issue\n",
    "test.loc[test.bsmt_exposure.isnull(), to_convert]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set looks good, no mislabeled entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary indicates an ordinal quality to these 5 variables. Will convert them to numerical values.\n",
    "\n",
    "Bsmt Qual (Ordinal): Evaluates the height of the basement\n",
    "\n",
    "       Ex\tExcellent (100+ inches)\t\n",
    "       Gd\tGood (90-99 inches)\n",
    "       TA\tTypical (80-89 inches)\n",
    "       Fa\tFair (70-79 inches)\n",
    "       Po\tPoor (<70 inches\n",
    "       NA\tNo Basement\n",
    "       \n",
    "Bsmt Cond (Ordinal): Evaluates the general condition of the basement\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical - slight dampness allowed\n",
    "       Fa\tFair - dampness or some cracking or settling\n",
    "       Po\tPoor - Severe cracking, settling, or wetness\n",
    "       NA\tNo Basement\n",
    "\t\n",
    "Bsmt Exposure\t(Ordinal): Refers to walkout or garden level walls\n",
    "\n",
    "       Gd\tGood Exposure\n",
    "       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n",
    "       Mn\tMimimum Exposure\n",
    "       No\tNo Exposure\n",
    "       NA\tNo Basement\n",
    "\t\n",
    "BsmtFin Type 1\t(Ordinal): Rating of basement finished area\n",
    "\n",
    "       GLQ\tGood Living Quarters\n",
    "       ALQ\tAverage Living Quarters\n",
    "       BLQ\tBelow Average Living Quarters\t\n",
    "       Rec\tAverage Rec Room\n",
    "       LwQ\tLow Quality\n",
    "       Unf\tUnfinshed\n",
    "       NA\tNo Basement\n",
    "\t\t\n",
    "BsmtFinType 2\t(Ordinal): Rating of basement finished area (if multiple types)\n",
    "\n",
    "       GLQ\tGood Living Quarters\n",
    "       ALQ\tAverage Living Quarters\n",
    "       BLQ\tBelow Average Living Quarters\t\n",
    "       Rec\tAverage Rec Room\n",
    "       LwQ\tLow Quality\n",
    "       Unf\tUnfinshed\n",
    "       NA\tNo Basement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert _qual and _cond categorical values to numerical values for train and test sets\n",
    "convert = {np.nan : 0,\n",
    "            'Po' : 1,\n",
    "            'Fa' : 2,\n",
    "            'TA' : 3,\n",
    "            'Gd' : 4,\n",
    "            'Ex' : 5\n",
    "}\n",
    "\n",
    "train.bsmt_qual = train.bsmt_qual.map(convert)\n",
    "test.bsmt_qual = test.bsmt_qual.map(convert)\n",
    "\n",
    "train.bsmt_cond = train.bsmt_cond.map(convert)\n",
    "test.bsmt_cond = test.bsmt_cond.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert _exposure categorical values to numerical values for train and test sets\n",
    "convert = {np.nan : 0,\n",
    "            'nan' : 0,\n",
    "            'No' : 1,\n",
    "            'Mn' : 2,\n",
    "            'Av' : 3,\n",
    "            'Gd' : 4,\n",
    "}\n",
    "\n",
    "train.bsmt_exposure = train.bsmt_exposure.map(convert)\n",
    "test.bsmt_exposure = test.bsmt_exposure.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert _exposure categorical values to numerical values for train and test sets\n",
    "convert = {np.nan : 0,\n",
    "            'Unf' : 1,\n",
    "            'LwQ' : 2,\n",
    "            'Rec' : 3,\n",
    "            'BLQ' : 4,\n",
    "            'ALQ' : 5,\n",
    "            'GLQ' : 6,\n",
    "}\n",
    "\n",
    "train.bsmtfin_type_1 = train.bsmtfin_type_1.map(convert)\n",
    "test.bsmtfin_type_1 = test.bsmtfin_type_1.map(convert)\n",
    "\n",
    "train.bsmtfin_type_2 = train.bsmtfin_type_2.map(convert)\n",
    "test.bsmtfin_type_2 = test.bsmtfin_type_2.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "\n",
    "for _ in to_convert:\n",
    "    print(f\"Train set: {_} has {train[_].isnull().sum()} null values.\")\n",
    "    print(f\"Test set: {_} has {test[_].isnull().sum()} null values.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Plot of Sales Price against Basement variables')\n",
    "\n",
    "y_variable = 'saleprice'\n",
    "\n",
    "x_variables = to_convert\n",
    "\n",
    "for i, axes in zip(x_variables, axes.flat):\n",
    "    sns.regplot(ax=axes, data=train, x=i, y=y_variable, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from string to number looks correct in general, except for bsmtfin_type_2 which doesn't seem to correlate with sale prices. Perhaps not unexpected, as type 2 finish is secondary to type 1 finish, and type 1 finish displays a positive relationship to sales price.\n",
    "\n",
    "Update tracking table for converted ordinal variables \n",
    "<a id='converted_ordinal_table_1'></a>\n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mas_vnr_area, mas_vnr_type\n",
    "\n",
    "To convert both at the same time as they have the same number of null entries, quite likely to be the same rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure of mas_vnr_type\n",
    "print(\"Unique values are\", train.mas_vnr_type.unique())\n",
    "print()\n",
    "print(\"Count of null values is\", np.sum(train.mas_vnr_type.isnull()))\n",
    "print()\n",
    "print(\"Count of non-null values by type \\n\", train.mas_vnr_type.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure of mas_vnr_area\n",
    "print(\"Count of null values is\", np.sum(train.lot_frontage.isnull()))\n",
    "print()\n",
    "print(\"The structure of non-null values is \\n\", train.lot_frontage.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if rows coincide\n",
    "to_convert = [\n",
    "    'mas_vnr_area', \n",
    "    'mas_vnr_type'\n",
    "]\n",
    "\n",
    "train.loc[train.mas_vnr_area.isnull(), to_convert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.mas_vnr_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.mas_vnr_type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The range of values for veneer type in the test set has one more value ('CBlock') that is not seen in the train set. This will pose a problem when predicting values for the test set as a parameter for a CBlock dummy would not be available. To consider removing this variable if not important later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile...Dictionary says NaN equates to no masonry veneer, so I shall fill all NaNs with \"None\". This variable is not ordinal as I can't see an obvious order. I guess different homeowners like different types of masonry, depending on their personal preferences.\n",
    "\n",
    "Mas Vnr Type (Nominal): Masonry veneer type\n",
    "\n",
    "       BrkCmn\tBrick Common\n",
    "       BrkFace\tBrick Face\n",
    "       CBlock\tCinder Block\n",
    "       None\tNone\n",
    "       Stone\tStone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mas_vnr_type, convert NaNs to None\n",
    "train.mas_vnr_type.fillna(\"None\", inplace = True)\n",
    "test.mas_vnr_type.fillna(\"None\", inplace = True)\n",
    "\n",
    "# For mas_vnr_area, convert NaNs to 0\n",
    "train.mas_vnr_area.fillna(0, inplace = True)\n",
    "test.mas_vnr_area.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1a\n",
    "train.mas_vnr_type.isnull().sum(), test.mas_vnr_type.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1b\n",
    "train.mas_vnr_area.isnull().sum(), test.mas_vnr_area.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2a\n",
    "sns.boxplot(data = train, x = \"mas_vnr_type\", y = \"saleprice\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2b\n",
    "sns.lmplot(data = train, x = \"mas_vnr_area\", y = \"saleprice\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train set, these remaining variables have 1-2 null values each:\n",
    "\n",
    "bsmt_half_bath, bsmt_full_bath, garage_area, total_bsmt_sf, bsmt_unf_sf, bsmtfin_sf_2, bsmtfin_sf_1, garage_cars\t\n",
    "\n",
    "For the test set, this remaining variable has 1 null value: electrical\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsmt_half_bath, bsmt_full_bath, garage_cars\n",
    "# Data dictionary shows these are discrete values. Set to 0 if NaN.\n",
    "\n",
    "train.bsmt_half_bath.fillna(0, inplace = True)\n",
    "train.bsmt_full_bath.fillna(0, inplace = True)\n",
    "train.garage_cars.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garage_area, total_bsmt_sf, bsmt_unf_sf, bsmtfin_sf_2, bsmtfin_sf_1\n",
    "# These are continuous variables in terms of square footage. Set to 0 if NaN.\n",
    "\n",
    "to_convert = ['garage_area', 'total_bsmt_sf', 'bsmt_unf_sf', 'bsmtfin_sf_2', 'bsmtfin_sf_1']\n",
    "for _ in to_convert:\n",
    "    train[_].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check electrical variable for test set\n",
    "test.loc[test.electrical.isnull(),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a big modern house so it would be wrong to set its electrical null value to zero or none. Will replace with the mode value for electrical, as regression imputation can't be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median value for electrical\n",
    "train.electrical.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute null value with median value for \"electrical\"\n",
    "test.loc[test.electrical.isnull(), \"electrical\"] = \"SBrkr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "train.electrical.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all columns and rows to 10 again, now that we are done with data cleaning, \n",
    "# so that we don't have to scroll through the entire list again\n",
    "\n",
    "pd.set_option('max_columns',10)\n",
    "pd.set_option('max_rows',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create columns for number of missing values and percent missing\n",
    "train_null = pd.DataFrame(\n",
    "    {'null_count' : train.isnull().sum(),\n",
    "    'non_null_count' : train.count()\n",
    "    }\n",
    ")\n",
    "\n",
    "train_null[\"percent_missing\"] = train_null.null_count / \\\n",
    "                                (train_null.null_count + train_null.non_null_count)\n",
    "\n",
    "train_null.sort_values(\"percent_missing\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, all rows when sorted in descending order have no null counts, except for lot_frontage, which is expected as the data-cleaned version of lot_frontage is saved as lot_frontage_imputed which has zero null count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "test_null = pd.DataFrame(\n",
    "    {'null_count' : test.isnull().sum(),\n",
    "    'non_null_count' : test.count()\n",
    "    }\n",
    ")\n",
    "\n",
    "test_null[\"percent_missing\"] = test_null.null_count / \\\n",
    "                                (test_null.null_count + test_null.non_null_count)\n",
    "\n",
    "test_null.sort_values(\"percent_missing\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, all rows when sorted in descending order have no null counts, except for lot_frontage, which is expected as the data-cleaned version of lot_frontage is saved as lot_frontage_imputed which has zero null count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (and 2nd round of feature engineering*)\n",
    "\\*removing features, combining categories, converting strings to numeric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "During data cleaning in Section 4.5 above, I converted some categorical variables from strings to integers ([latest conversion table here](#converted_ordinal_table_1)). In this section, I shall analyse the remaining categorical variables to see if conversion is suitable. \n",
    "\n",
    "In feature engineering, there are pros and cons with regards to conversion of descriptive entries to numerical values. Assigning an arbitrary \"0-1-2-3\" scale to \"poor-average-good-excellent\" make a strong assumption that there is an equal distance or difference between the various qualities, which may not be true in real life. For example, it may be extremely difficult to achieve an \"excellent\" rating, which it may be twice as \"good\" as a good rating, in which case a scale of \"0-1-2-4\" might be more reflective of reality than \"0-1-2-3\". To avoid making assumptions, one might perform one-hot encoding on categorical variables. In this case, we let the model assume and fit a natural ordering between the categories of quality, but the disadvantage is that it may result in poor performance or unexpected results (e.g. if the data set is not large or robust enough, a fitted model may assume \"excellent\" is worse than \"good\". In such cases, telling the model what the natural ordered relationship should be will allow the learning algorithm to harness this relationship more efficiently, i.e. we don't throw away good information that we know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot box plots of all categoricals first\n",
    "[Data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt) says there are a total of 46 categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull list of all categorical variables from data dictionary\n",
    "\n",
    "categorical = ['pid', 'ms_subclass', 'ms_zoning',\n",
    "       'street', 'alley', 'lot_shape', 'land_contour', 'utilities',\n",
    "       'lot_config', 'land_slope', 'neighborhood', 'condition_1',\n",
    "       'condition_2', 'bldg_type', 'house_style', 'overall_qual',\n",
    "       'overall_cond', 'roof_style',\n",
    "       'roof_matl', 'exterior_1st', 'exterior_2nd', 'mas_vnr_type',\n",
    "       'exter_qual', 'exter_cond', 'foundation', 'bsmt_qual',\n",
    "       'bsmt_cond', 'bsmt_exposure', 'bsmtfin_type_1', \n",
    "       'bsmtfin_type_2',\n",
    "       'heating', 'heating_qc', 'central_air', 'electrical', \n",
    "       'kitchen_qual', 'functional',\n",
    "       'fireplace_qu', 'garage_type', \n",
    "       'garage_finish', 'garage_qual',\n",
    "       'garage_cond', 'paved_drive', \n",
    "       'pool_qc',\n",
    "       'fence', 'misc_feature', 'sale_type',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length of categorical list to make sure I copied over correctly from data dictionary\n",
    "len(categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46 -> match the number of categorical variables described in data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all! Apologies, it takes a while to run...\n",
    "\n",
    "# Decided to save the .png file in ../images for us to review at our leisure anytime\n",
    "\n",
    "# fig, ax = plt.subplots(nrows = len(categorical), \n",
    "#                        ncols = 1,\n",
    "#                        figsize = (20, 200),\n",
    "#                       # sharey = True\n",
    "#                       )\n",
    "\n",
    "# ax = ax.ravel()\n",
    "# for i in range(len(categorical)):\n",
    "#     sns.boxplot(x = categorical[i], \n",
    "#                 y = 'saleprice',\n",
    "#                 data = train, \n",
    "#                 ax = ax[i],\n",
    "#                 palette = \"pastel\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Boxplots of all categorical variables in Train set - First Cut](../images/train_categorical_boxplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My observations and proposed course of action list in the table below. \n",
    "\n",
    "Some variables are dropped because visually all categories are similar from one another, so they would not be useful in predicting sale prices and will just add noise (and root mean squared error!) instead. Some variables will be converted to numeric values as they exhibit a clear order in terms of average sale prices, so we want to keep the relationship as much as possible. A total of 25 categorical variables are retained after this round of trim and cut. \n",
    "\n",
    "<a id='notes_from_box_plots'></a>\n",
    "\n",
    "| Variable | Observations | Keep (Y/N) | Remarks\n",
    "|----------|-------------------------------------------|----------------------------------|-----|\n",
    "| pid | Not useful as categorical variable as every value is unique | N | \n",
    "| ms_subclass | Not a lot of variation between categories, and for those with variation (60, 120), they are highly correlated with year_built as seen in pairplot chart in Section 3.4 | N |\n",
    "| ms_zoning | Same as above, not a lot of variation | N |\n",
    "| street | To keep, paved is clearly ranked above gravel | Y | Convert to dummy where Paved is 1 and else is 0 |\n",
    "| alley | No obvious variation between categories | N |\n",
    "| lot_shape | No obvious variation between categories  | N |\n",
    "| land_contour | No obvious variation between categories | N |\n",
    "| utilities | No obvious variation between categories | N |\n",
    "| lot_config | No obvious variation between categories | N |\n",
    "| land_slope | No obvious variation between categories | N |\n",
    "| neighborhood | Big differences between neighborhoods, e.g. StoneBr and NridgHt seem to be the top-tier neighborhoods while IDOTRR and MedowV are at the bottom| Y | Remember to create dummies for GrnHill and Landmrk in the test set, as done in regression imputation for lot_frontage |\n",
    "| condition_1 | Those that are near or adjacent to positive off-site features (PosN, PosA) are clearly above Normal, which is in turn clearly above those near the railroad (RRNn, RRAn, RRNe, RRAe) and small roads (Artery, Feedr) | Y | Candidate for collapsing 9 categories into 3 or 4 combined categories |\n",
    "| condition_2 | Same as above, to keep | Y | Same as above\n",
    "| bldg_type | No obvious variation between categories | N |\n",
    "| house_style | No obvious variation between categories | N |\n",
    "| overall_qual | Strong relationship between increasing levels of quality and increasing sale prices | Y |\n",
    "| overall_cond | Difficult one, as the ordering does not make sense: \"5\" has a higher average sales price than \"1-4\" and \"6-10\". It looks likes a victim of a poorly-phrased survey question, or most people just lazily impute \"5\" even if a house is in a good condition. Should drop this variable as it will confuse the model,  resulting in poor performance or unexpected results  | N |\n",
    "| roof_style | No obvious variation between categories | N |\n",
    "| roof_matl | No obvious variation between categories | N |\n",
    "| exterior_1st | No obvious variation between categories | N |\n",
    "| exterior_2nd | No obvious variation between categories | N |\n",
    "| mas_vnr_type | Clear variation betwen stone and brick face, versus the rest | Y | Possible to combine BrkCmn and None into one category as they have similar distributions |\n",
    "| exter_qual | Clear sales price order from Poor to Excellent | Y | Convert descriptive strings to numeric\n",
    "| exter_cond | Clear sales price order from Poor to Excellent | Y | Convert descriptive strings to numeric\n",
    "| foundation | Poured concrete is stands heads and shoulders above the other categories which do not exhibit variation from one another | Y | Possible to convert to a dummy where PConc is 1 and the rest are 0 |\n",
    "| bsmt_qual | Ordering is clear | Y | \n",
    "| bsmt_cond | Unclear ordering, possibility to confuse regreesion model as better condition does not signify better prices | N | \n",
    "| bsmt_exposure | Ordering is clear | Y | \n",
    "| bsmtfin_type_1 | Ordering is clear | Y | \n",
    "| bsmtfin_type_2 | Ordering is clear | Y | \n",
    "| heating | GasA and GasW stands heads and shoulders above the other categories which do not exhibit variation from one another | Y | Possible to convert to a dummy where GasA and GasW are 1 and the rest are 0 |\n",
    "| heating_qc | Ordering is clear from Poor to Excellent | Y | Convert descriptive strings to numeric\n",
    "| central_air | Clear that houses with central air conditioning command higher prices (mean, and most percentile levels) than those without | Y | Convert to dummy where Y is 1 and N is 0 |\n",
    "| electrical | Those with standard circuit breakers (SBrkr) command higher prices than those with non-standard systems | Y | Convert to dummy where SBrkr is 1 and all other 4 categories are mapped to 0 |\n",
    "| kitchen_qual | Ordering is clear from Fair to Excellent | Y | Convert descriptive strings to numeric |\n",
    "| functional | Ordering from Sal -> Sev -> ... -> Typ | Y | Convert descriptive strings to numeric |\n",
    "| fireplace_qu | Ordering is clear | Y | \n",
    "| garage_type | Clear that those with \"none\" for garage ranks lowest in terms of price, but the other categories do not seem to exhibit strong variation with one another. To drop, as the lack of garage can already be teased out in other variables such as garage_finish and garage_qual where 0 means no garage | N | \n",
    "| garage_finish | Ordering is clear | Y | \n",
    "| garage_qual | Ordering is clear | Y | To check if it is highly correlated to garage_finish\n",
    "| garage_cond | Ordering is clear | Y | To check if it is highly correlated to garage_finish and garage_qual\n",
    "| paved_drive | Ordering is clear, from N -> P -> Y in increasing prices | Y | Convert descriptive strings to numeric |\n",
    "| pool_qc | No clear pattern between categories, except for the fact that houses with pools command higher prices than those without. But this fact can be exhibit in pool_area with 0 equates to houses without pools. To drop. | N | \n",
    "| fence | No obvious variation between categories | N |\n",
    "| misc_feature | No obvious variation between categories | N |\n",
    "| sale_type | Variations between categories, especially with New above the rest. No obvious way to combine categories, to leave as is | Y |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to conduct feature engineering on categoricals that I wish to keep, as described in the \"Remarks\" column in the table above, but first, start creating an empty \"final\" dataframe that I wish to keep and save as csv, and gradually append columns that I wish to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create fresh df to start storing chosen variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df\n",
    "df_train = pd.DataFrame(train['saleprice'], columns = {\"saleprice\"})\n",
    "df_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Pave\" to 1 and \"Grvl\" to 0\n",
    "df_train['street'] = train['street'].map(lambda x: 1 if x == \"Pave\" else 0)\n",
    "df_test['street'] = test['street'].map(lambda x: 1 if x == \"Pave\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave, Gravel | 1, 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: Convert neighborhood to dummies\n",
    "\n",
    "# Do not use drop_first = True so that I can choose the same neighborhood to drop for\n",
    "# both Train and Test sets. I randomly pick \"Old Town\" as the dummy to drop for both sets.\n",
    "\n",
    "df_train = pd.merge(\n",
    "    left = df_train,\n",
    "    right = pd.get_dummies(\n",
    "        data = train['neighborhood'], \n",
    "        prefix = 'neighborhood',\n",
    "        drop_first = False\n",
    "    ).drop(\n",
    "        columns = {'neighborhood_OldTown'}\n",
    "    ),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks right, with (28 minus 1) neighborhoods added to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Convert neighborhood to dummies,\n",
    "#       and remember to add the two missing neighborhoods in test set\n",
    "\n",
    "df_test = pd.merge(\n",
    "    left = df_test,\n",
    "    right = pd.get_dummies(\n",
    "        data = test['neighborhood'], \n",
    "        prefix = 'neighborhood',\n",
    "        drop_first = False\n",
    "    ).drop(\n",
    "        columns = {'neighborhood_OldTown'}\n",
    "    ),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "# Add neighborhoods found in Train set but not in Test set\n",
    "df_test[['neighborhood_GrnHill', 'neighborhood_Landmrk']] = 0\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks right, with (28 minus 1) neighborhoods added to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave, Gravel | 1, 0 |\n",
    "| neighborhood | 28 neighbourhoods | One-Hot-Encoding, drop \"OldTown\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### condition_1 \n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Those that are near or adjacent to positive off-site features (PosN, PosA) are clearly above Normal, which is in turn clearly above those near the railroad (RRNn, RRAn, RRNe, RRAe) and small roads (Artery, Feedr) | Y | Candidate for collapsing 9 categories into 3 or 4 combined categories |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\",train.condition_1.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.condition_1.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks sensible to combine the non-\"Norm\" categories as their value counts are small individually, so that we don't have too many rare categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference dictionary to collapse 9 into 4 categories\n",
    "convert = {\n",
    "    'Norm' : 'Norm',\n",
    "    'Feedr' : 'FeedArt',\n",
    "    'Artery' : 'FeedArt',\n",
    "    'RRAn' : 'Rail',\n",
    "    'RRAe' : 'Rail',\n",
    "    'RRNn' : 'Rail',\n",
    "    'RRNe' : 'Rail',\n",
    "    'PosN' : 'Pos',\n",
    "    'PosA' : 'Pos'\n",
    "}\n",
    "\n",
    "# Combine and create dummies, manually drop \"Norm\" so that it becomes the baseline dummy\n",
    "df_train = pd.merge(\n",
    "    left = df_train,\n",
    "    right = pd.get_dummies(\n",
    "        data = train.condition_1.map(convert), \n",
    "        prefix = 'condition_1',\n",
    "        drop_first = False\n",
    "    ).drop(\n",
    "        columns = {'condition_1_Norm'}\n",
    "    ),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set, drop \"Norm\" to become the baseline dummy\n",
    "\n",
    "df_test = pd.merge(\n",
    "    left = df_test,\n",
    "    right = pd.get_dummies(\n",
    "        data = test.condition_1.map(convert), \n",
    "        prefix = 'condition_1',\n",
    "        drop_first = False\n",
    "    ).drop(\n",
    "        columns = {'condition_1_Norm'}\n",
    "    ),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave, Gravel | 1, 0 |\n",
    "| neighborhood | 28 neighbourhoods | One-Hot-Encoding, drop \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | One-Hot-Encoding, drop \"Norm\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### condition_2\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "Same as above, to keep | Y | Same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\",round(train.condition_2.value_counts(normalize = True),4))\n",
    "print()\n",
    "print(\"Test set:\\n\", round(test.condition_2.value_counts(normalize = True),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of non-\"Norm\" variables in condition_2 looks very sparse, less than 2% of the sample size, unlike condition_1. It is unlikely to generate any significant inferences for sale prices and probably just increase noise in the model. Will drop this variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overall_qual \n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Strong relationship between increasing levels of quality and increasing sale prices | Y |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.overall_qual.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.overall_qual.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm relationship\n",
    "sns.lmplot(x = \"overall_qual\", y = \"saleprice\", data = train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variable to dfs\n",
    "df_train['overall_qual'] = train['overall_qual']\n",
    "df_test['overall_qual'] = test['overall_qual']\n",
    "\n",
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mas_vnr_type \n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Clear variation betwen stone and brick face, versus the rest | Y | Possible to combine BrkCmn and None into one category as they have similar distributions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.mas_vnr_type .value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.mas_vnr_type .value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = train, x = \"saleprice\", hue = \"mas_vnr_type\", kde = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"None\" is the largest category and can be the baseline for dummies. BrkFace and Stone should be different categories as they have different peaks. BrkCmn can be merged into None as their boxplots are visually not different from each other. Test set has an entry called \"CBlock\" that is not seen in the Train set, we can't do anything about it as our model can't be trained on a value that is not available, so will merge CBlock into None in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference dictionary to collapse 9 into 4 categories\n",
    "convert = {\n",
    "    'None' : 'None',\n",
    "    'CBlock' : 'None',\n",
    "    'BrkCmn' : 'None',\n",
    "    'BrkFace' : 'BrkFace',\n",
    "    'Stone' : 'Stone',\n",
    "}\n",
    "\n",
    "# Combine and create dummies, manually drop \"None\" so that it becomes the baseline dummy\n",
    "df_train = pd.merge(\n",
    "    left = df_train,\n",
    "    right = pd.get_dummies(\n",
    "        data = train.mas_vnr_type.map(convert), \n",
    "        prefix = 'mas_vnr_type',\n",
    "        drop_first = False\n",
    "    ).drop(\n",
    "        columns = {'mas_vnr_type_None'}\n",
    "    ),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for Test, manually drop \"None\" so that it becomes the baseline dummy\n",
    "df_test = pd.merge(\n",
    "    left = df_test,\n",
    "    right = pd.get_dummies(\n",
    "        data = test.mas_vnr_type.map(convert), \n",
    "        prefix = 'mas_vnr_type',\n",
    "        drop_first = False\n",
    "    ).drop(\n",
    "        columns = {'mas_vnr_type_None'}\n",
    "    ),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave, Gravel | 1, 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exter_qual\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Clear sales price order from Poor to Excellent | Y | Convert descriptive strings to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.exter_qual.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.exter_qual.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to numerical values\n",
    "convert = {'Fa' : 0,\n",
    "           'TA' : 1,\n",
    "           'Gd' : 2,\n",
    "           'Ex' : 3\n",
    "}\n",
    "\n",
    "df_train['exter_qual'] = train.exter_qual.map(convert)\n",
    "df_test['exter_qual'] = test.exter_qual.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.lmplot(x = \"exter_qual\", y = \"saleprice\", data = df_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave, Gravel | 1, 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exter_cond\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Clear sales price order from Poor to Excellent | Y | Convert descriptive strings to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.exter_cond.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.exter_cond.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to numerical values\n",
    "convert = {'Po' : 0,\n",
    "           'Fa' : 1,\n",
    "           'TA' : 2,\n",
    "           'Gd' : 3,\n",
    "           'Ex' : 4\n",
    "}\n",
    "\n",
    "df_train['exter_cond'] = train.exter_cond.map(convert)\n",
    "df_test['exter_cond'] = test.exter_cond.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.lmplot(x = \"exter_cond\", y = \"saleprice\", data = df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = \"exter_cond\", y = \"saleprice\", data = df_train, order = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...upon closer look after converting the ordinal categories to numeric values, the relationship does not seem to be strong between saleprice and exter_cond anymore. The line seems to be \"flattened out\" by lower-than-expected sale prices for categories 4 (Excellent) and 3 (Good), or messed up by the fact that too many houses, including good and excellent ones, are in the \"Average\" category. In fact, if i relax the regression to allow for order=2, it seems to suggest Good and Excellent houses are priced cheaper than Average houses. Will remove the variable from our final dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove exter_cond\n",
    "df_train.drop(columns = ['exter_cond'], inplace = True)\n",
    "df_test.drop(columns = ['exter_cond'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foundation\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Poured concrete is stands heads and shoulders above the other categories which do not exhibit variation from one another | Y | Possible to convert to a dummy where PConc is 1 and the rest are 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.foundation.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.foundation.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.displot(data = train, x = \"saleprice\", hue = \"foundation\", kind = \"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour concrete (PConc) has a higher mean and more right-skewed than the rest, which has quite similar means and standard deviations. Other categories such as Slab, Stone and Wood also have very small sample sizes, which make them good candidates to merge to other categories that behave similarly. \n",
    "\n",
    "Create simple dummy where PConc = 1 and the rest are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create foundation dummy\n",
    "df_train['foundation'] = train.foundation.map(lambda x: 1 if x == \"PConc\" else 0)\n",
    "df_test['foundation'] = train.foundation.map(lambda x: 1 if x == \"PConc\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave, Gravel | 1, 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |\n",
    "| foundation | PConc, CBlock, BrkTil, Slab, Stone, Wood  | PConc = 1, all others = 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bsmt_qual\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear | Y | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.bsmt_qual.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.bsmt_qual.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm relationship\n",
    "sns.lmplot(x = \"bsmt_qual\", y = \"saleprice\", data = train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variable to dfs\n",
    "df_train['bsmt_qual'] = train['bsmt_qual']\n",
    "df_test['bsmt_qual'] = test['bsmt_qual']\n",
    "\n",
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bsmt_exposure\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear | Y | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.bsmt_exposure.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.bsmt_exposure.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm relationship\n",
    "sns.lmplot(x = \"bsmt_exposure\", y = \"saleprice\", data = train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variable to dfs\n",
    "df_train['bsmt_exposure'] = train['bsmt_exposure']\n",
    "df_test['bsmt_exposure'] = test['bsmt_exposure']\n",
    "\n",
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bsmtfin_type_1\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear | Y | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.bsmtfin_type_1.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.bsmtfin_type_1.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm relationship\n",
    "sns.lmplot(x = \"bsmtfin_type_1\", y = \"saleprice\", data = train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variable to dfs\n",
    "df_train['bsmtfin_type_1'] = train['bsmtfin_type_1']\n",
    "df_test['bsmtfin_type_1'] = test['bsmtfin_type_1']\n",
    "\n",
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bsmtfin_type_2\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear | Y | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.bsmtfin_type_2.value_counts())\n",
    "print()\n",
    "print(\"Test set:\\n\", test.bsmtfin_type_2.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm relationship\n",
    "sns.lmplot(x = \"bsmtfin_type_2\", y = \"saleprice\", data = train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['saleprice', 'bsmtfin_type_2']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems flattish, exclude from final dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### heating\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| GasA and GasW stands heads and shoulders above the other categories which do not exhibit variation from one another | Y | Possible to convert to a dummy where GasA and GasW are 1 and the rest are 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.heating.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.heating.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon closer look, even though GasA and GasW stand clearly above the other heating categories, i do not think it is useful to add this variable into our regression models as GasA and GasW make up 99% of the observations!! It wouldn't aid in improving model learning by passing a column of mostly 1s to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### heating_qc\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear from Poor to Excellent | Y | Convert descriptive strings to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.heating_qc.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.heating_qc.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heating_qc looks more promising than heating above. Will run it through the string-to-numeric conversion and check for positive relationship later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to numerical values\n",
    "convert = {'Po' : 0,\n",
    "           'Fa' : 1,\n",
    "           'TA' : 2,\n",
    "           'Gd' : 3,\n",
    "           'Ex' : 4\n",
    "}\n",
    "\n",
    "df_train['heating_qc'] = train.heating_qc.map(convert)\n",
    "df_test['heating_qc'] = test.heating_qc.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.lmplot(x = \"heating_qc\", y = \"saleprice\", data = df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['saleprice', 'heating_qc']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "| heating_qc | Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave, Gravel | 1, 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |\n",
    "| foundation | PConc, CBlock, BrkTil, Slab, Stone, Wood  | PConc = 1, all others = 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### central_air \n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Clear that houses with central air conditioning command higher prices (mean, and most percentile levels) than those without | Y | Convert to dummy where Y is 1 and N is 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.central_air.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.central_air.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as bad as \"heating\" which was dropped. At least the houses without central airconditioning makes up more than 5% of the observations. To convert into dummy where Y = 1 and N = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.displot(data = train, x = \"saleprice\", hue = \"central_air\", kind = \"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirms that the two distributions are quite different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create central_air dummy\n",
    "df_train['central_air'] = train.central_air.map(lambda x: 1 if x == \"Y\" else 0)\n",
    "df_test['central_air'] = train.central_air.map(lambda x: 1 if x == \"Y\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "| heating_qc | Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave - Gravel | 1 - 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |\n",
    "| foundation | PConc, CBlock, BrkTil, Slab, Stone, Wood  | PConc = 1, all others = 0 |\n",
    "| central_air | Y - N | 1 - 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### electrical\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Those with standard circuit breakers (SBrkr) command higher prices than those with non-standard systems | Y | Convert to dummy where SBrkr is 1 and all other 4 categories are mapped to 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.electrical.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.electrical.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.displot(data = train, x = \"saleprice\", hue = \"electrical\", kind = \"kde\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(train.groupby(\"electrical\").saleprice.describe(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price distributions do look different for SBrkr versus the rest. Will create dummy where SBrkr = 1 and the rest = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create electrical dummy\n",
    "df_train['electrical'] = train.electrical.map(lambda x: 1 if x == \"SBrkr\" else 0)\n",
    "df_test['electrical'] = train.electrical.map(lambda x: 1 if x == \"SBrkr\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "| heating_qc | Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave - Gravel | 1 - 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |\n",
    "| foundation | PConc, CBlock, BrkTil, Slab, Stone, Wood  | PConc = 1, all others = 0 |\n",
    "| central_air | Y - N | 1 - 0 |\n",
    "| electrical | SBrkr, FuseA, FuseF, FuseP, Mix | SBrkr = 1, all others = 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kitchen_qual\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear from Fair to Excellent | Y | Convert descriptive strings to numeric |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.kitchen_qual.value_counts(normalize=False))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.kitchen_qual.value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that test set has \"Po\" as a value while train does not. Model can't predict sale price on a value it does not see in the train set. Group Po and Fa together. It is okay because we are only looking at 1 datapoint for \"Po\", out of 800+ datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to numerical values\n",
    "convert = {'Po' : 0,\n",
    "           'Fa' : 0,\n",
    "           'TA' : 1,\n",
    "           'Gd' : 2,\n",
    "           'Ex' : 3\n",
    "}\n",
    "\n",
    "df_train['kitchen_qual'] = train.kitchen_qual.map(convert)\n",
    "df_test['kitchen_qual'] = test.kitchen_qual.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2\n",
    "sns.lmplot(x = \"kitchen_qual\", y = \"saleprice\", data = df_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tracking table for converted ordinal variables \n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "| heating_qc | Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| kitchen_qual | Po -> Fa -> TA -> Gd -> Ex | 0-0-1-2-3 |\n",
    "\n",
    "Update tracking table for converted dummy variables \n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave - Gravel | 1 - 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |\n",
    "| foundation | PConc, CBlock, BrkTil, Slab, Stone, Wood  | PConc = 1, all others = 0 |\n",
    "| central_air | Y - N | 1 - 0 |\n",
    "| electrical | SBrkr, FuseA, FuseF, FuseP, Mix | SBrkr = 1, all others = 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functional\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering from Sal -> Sev -> ... -> Typ | Y | Convert descriptive strings to numeric |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.functional.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.functional.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of converting Sal -> Sev -> ... -> Typ to a numeric range, it might be better to just create a dummy to indicate Typ = 1 and group the rest into 0 because they are too fragmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.displot(data = train, x = \"saleprice\", hue = \"functional\", kind = \"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too hard to see anything above. Use .groupby.describe() instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(train.groupby(\"functional\").saleprice.describe(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group all non-Typ together as they are on average worse than Typ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create electrical dummy\n",
    "df_train['functional'] = train.functional.map(lambda x: 1 if x == \"Typ\" else 0)\n",
    "df_test['functional'] = train.functional.map(lambda x: 1 if x == \"Typ\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Update tracking table for converted ordinal variables_\n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "| heating_qc | Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| kitchen_qual | Po -> Fa -> TA -> Gd -> Ex | 0-0-1-2-3 |\n",
    "\n",
    "\n",
    "_Update tracking table for converted dummy variables_\n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave - Gravel | 1 - 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |\n",
    "| foundation | PConc, CBlock, BrkTil, Slab, Stone, Wood  | PConc = 1, all others = 0 |\n",
    "| central_air | Y - N | 1 - 0 |\n",
    "| electrical | SBrkr, FuseA, FuseF, FuseP, Mix | SBrkr = 1, all others = 0 |\n",
    "| functional | Typ, Min1, Min2, Mod, Maj1, Maj2, Sev, Sal | Typ = 1, all others = 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fireplace_qu\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear | Y | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.fireplace_qu.value_counts(normalize=False))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.fireplace_qu.value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.lmplot(x = \"fireplace_qu\", y = \"saleprice\", data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variable to dfs\n",
    "df_train['fireplace_qu'] = train['fireplace_qu']\n",
    "df_test['fireplace_qu'] = test['fireplace_qu']\n",
    "\n",
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### garage_finish\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear | Y | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.garage_finish.value_counts(normalize=False))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.garage_finish.value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.lmplot(x = \"garage_finish\", y = \"saleprice\", data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variable to dfs\n",
    "df_train['garage_finish'] = train['garage_finish']\n",
    "df_test['garage_finish'] = test['garage_finish']\n",
    "\n",
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### garage_qual\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear | Y | To check if it is highly correlated to garage_finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.garage_qual.value_counts(normalize=False))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.garage_qual.value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.lmplot(x = \"garage_qual\", y = \"saleprice\", data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variable to dfs\n",
    "df_train['garage_qual'] = train['garage_qual']\n",
    "df_test['garage_qual'] = test['garage_qual']\n",
    "\n",
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### garage_cond \n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear | Y | To check if it is highly correlated to garage_finish and garage_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.garage_cond.value_counts(normalize=False))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.garage_cond.value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.lmplot(x = \"garage_cond\", y = \"saleprice\", data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variable to dfs\n",
    "df_train['garage_cond'] = train['garage_cond']\n",
    "df_test['garage_cond'] = test['garage_cond']\n",
    "\n",
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### paved_drive\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Ordering is clear, from N -> P -> Y in increasing prices | Y | Convert descriptive strings to numeric |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.paved_drive.value_counts(normalize=False))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.paved_drive.value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.displot(x = \"saleprice\", hue = \"paved_drive\", data = train, kind = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paved (Y) looks quite different from Partial Pavement (P) and Dirt/Gravel (N). The latter two looks similar in mean and stdev, so can combine into one category.\n",
    "\n",
    "Create dummy where Y = 1, P = 0, N = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create electrical dummy\n",
    "df_train['paved_drive'] = train.paved_drive.map(lambda x: 1 if x == \"Y\" else 0)\n",
    "df_test['paved_drive'] = train.paved_drive.map(lambda x: 1 if x == \"Y\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Update tracking table for converted ordinal variables_\n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "| heating_qc | Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| kitchen_qual | Po -> Fa -> TA -> Gd -> Ex | 0-0-1-2-3 |\n",
    "\n",
    "\n",
    "_Update tracking table for converted dummy variables_\n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave - Gravel | 1 - 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |\n",
    "| foundation | PConc, CBlock, BrkTil, Slab, Stone, Wood  | PConc = 1, all others = 0 |\n",
    "| central_air | Y - N | 1 - 0 |\n",
    "| electrical | SBrkr, FuseA, FuseF, FuseP, Mix | SBrkr = 1, all others = 0 |\n",
    "| functional | Typ, Min1, Min2, Mod, Maj1, Maj2, Sev, Sal | Typ = 1, all others = 0 |\n",
    "| paved_drive | Y - P - N | 1 - 0 - 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sale_type\n",
    "[Notes from boxplots](#notes_from_box_plots):\n",
    "| Variations between categories, especially with New above the rest. No obvious way to combine categories, to leave as is | Y |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "print(\"Train set:\\n\", train.sale_type.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test set:\\n\", test.sale_type.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper dive\n",
    "sns.displot(data = train, x = \"saleprice\", hue = \"sale_type\", kind = \"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like i can see 3 distinct distributions above (WD, New, COD) and the rest are insignificant. \n",
    "Use .groupby.describe() to look in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(train.groupby(\"sale_type\").saleprice.describe(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might be sensible to collapse into a 4 categories and dummify them: WD is conventional so can be baseline dummy, COD has significantly lower mean and reasonable count, New is significantly higher than all other categories and has reasonable count so it can stand on its own, all others can be collapsed into a category called \"Others\". Our goal is to capture the possibly-significant differences in COD and New."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference dictionary to collapse 9 into 4 categories\n",
    "convert = {\n",
    "    'WD ' : 'WD',\n",
    "    'COD' : 'COD',\n",
    "    'New' : 'New',\n",
    "    'CWD' : 'Others',\n",
    "    'Con' : 'Others',\n",
    "    'ConLD' : 'Others',\n",
    "    'ConLI' : 'Others',\n",
    "    'ConLw' : 'Others',\n",
    "    'Oth' : 'Others',\n",
    "}\n",
    "\n",
    "# Combine and create dummies, manually drop \"WD\" so that it becomes the baseline dummy\n",
    "df_train = pd.merge(\n",
    "    left = df_train,\n",
    "    right = pd.get_dummies(\n",
    "        data = train.sale_type.map(convert), \n",
    "        prefix = 'sale_type',\n",
    "        drop_first = False\n",
    "    ).drop(\n",
    "        columns = {'sale_type_WD'}\n",
    "    ),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test, manually drop \"WD\" so that it becomes the baseline dummy\n",
    "df_test = pd.merge(\n",
    "    left = df_test,\n",
    "    right = pd.get_dummies(\n",
    "        data = test.sale_type.map(convert), \n",
    "        prefix = 'sale_type',\n",
    "        drop_first = False\n",
    "    ).drop(\n",
    "        columns = {'sale_type_WD'}\n",
    "    ),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='converted_ordinal_table'></a>\n",
    "_Finalised table for converted ordinal variables_\n",
    "\n",
    "| Variable | Original Categories (in order of increasing quality) | Replacement Numbers (higher is better)| \n",
    "|----------|-------------------------------------------|---------------------------------------|\n",
    "| pool_qc | NaN -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| fireplace_qu | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_finish | NaN -> Unf -> RFn -> Fin | 0-1-2-3 |\n",
    "| garage_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| garage_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_cond | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_qual | NaN -> Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4-5 |\n",
    "| bsmt_exposure | NaN -> No -> Mn -> Av -> Gd | 0-1-2-3-4 |\n",
    "| bsmtfin_type_1 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| bsmtfin_type_2 | NaN -> Unf -> LwQ -> Rec -> BLW -> ALQ -> GLQ | 0-1-2-3-4-5-6 |\n",
    "| exter_qual | Fa -> TA -> Gd -> Ex | 0-1-2-3 | \n",
    "| heating_qc | Po -> Fa -> TA -> Gd -> Ex | 0-1-2-3-4 | \n",
    "| kitchen_qual | Po -> Fa -> TA -> Gd -> Ex | 0-0-1-2-3 |\n",
    "\n",
    "\n",
    "_Finalised tracking table for converted dummy variables_\n",
    "\n",
    "| Variable | Original Categories | Replacement Numbers |\n",
    "|----------|---------------------|---------------------|\n",
    "| street | Pave - Gravel | 1 - 0 |\n",
    "| neighborhood | 28 neighbourhoods | 27 dummies, dropped \"OldTown\" |\n",
    "| condition_1 | Collapse into \"Norm\", \"FeederArt\", \"Pos\", \"Rail\" | 3 dummies, dropped \"Norm\" |\n",
    "| mas_vnr_type | Collapse into \"None\", \"BrkFace\", \"Stone\" | 2 dummies, dropped \"None\" |\n",
    "| foundation | PConc, CBlock, BrkTil, Slab, Stone, Wood  | PConc = 1, all others = 0 |\n",
    "| central_air | Y - N | 1 - 0 |\n",
    "| electrical | SBrkr, FuseA, FuseF, FuseP, Mix | SBrkr = 1, all others = 0 |\n",
    "| functional | Typ, Min1, Min2, Mod, Maj1, Maj2, Sev, Sal | Typ = 1, all others = 0 |\n",
    "| paved_drive | Y - P - N | 1 - 0 - 0 |\n",
    "| sale_type | Collapse into \"WD\", \"COD\", \"New\", \"Others\" | 3 dummies, dropped \"WD\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selected and transformed categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dummies and ordinal integers\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dummies and ordinal integers\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for outliers\n",
    "[Data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt) says there are a total of 34 non-categorical / numeric variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull list of all categorical variables from data dictionary\n",
    "\n",
    "# Use this to obtain initial list, then drill down to the correct ones: \n",
    "#  train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "numeric = ['lot_frontage_imputed', \n",
    "           'year_built', 'year_remod/add', 'mas_vnr_area',\n",
    "           'bsmtfin_sf_1',\n",
    "           'bsmtfin_sf_2', 'bsmt_unf_sf',\n",
    "           'total_bsmt_sf', '1st_flr_sf', '2nd_flr_sf', 'low_qual_fin_sf',\n",
    "           'gr_liv_area', 'bsmt_full_bath', 'bsmt_half_bath', 'full_bath',\n",
    "           'half_bath', 'bedroom_abvgr', 'kitchen_abvgr', 'totrms_abvgrd',\n",
    "           'fireplaces', 'garage_cars',\n",
    "           'garage_area', 'wood_deck_sf',\n",
    "           'open_porch_sf', 'enclosed_porch', '3ssn_porch', 'screen_porch',\n",
    "           'pool_area', 'misc_val', 'mo_sold', 'yr_sold', 'saleprice'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 33 because House ID is dropped as it is discrete but not a useful variable for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create function to spot outliers based on deviation from the mean\n",
    "\n",
    "def outliers(data, columns_to_check, sd_threshold = 3):\n",
    "    for _ in columns_to_check:\n",
    "        print(\n",
    "            data.loc[(\n",
    "                abs(data[_] - np.mean(data[_])) >\\\n",
    "                np.std(data[_]) * sd_threshold\n",
    "            ),\n",
    "                [_]\n",
    "            ]\n",
    "        )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for numeric outliers in Train set that are more than 3 standard deviations away from the mean, as 99.75% of the values should lie within 3 SD.....if the distribution is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Train set\n",
    "outliers(train, numeric, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Test set\n",
    "outliers(test, numeric[:-1], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My eyeballs can't take it...too many values lie outside 3 SD. Housing variables clearly don't follow normal distributions!\n",
    "\n",
    "Let's increase the threshold to 5 SD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Train set\n",
    "outliers(train, numeric, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check Test set, exlude \"saleprice\" from columns_to_check argument\n",
    "outliers(test, numeric[:-1], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see the same index numbers appearing in the results above for Train set. This indicates that the values may not be wrongly entered in the columns, large unusual features can possibly exist.\n",
    "Nevertheless, in the pairplots during [inital data appreciation](#initial_appreciation_of_data), I note some outliers with sale prices that may not represent true market value. These are the relevant extracted notes from above:\n",
    "\n",
    "- Lot frontage: Slightly upward trend where larger lot frontage equates to higher prices. 2 potential outliers where lot frontage are extremely large but prices are low\n",
    "- Lot area: Same issue with 2 possible outliers where transacted prices look too low for the lot sizes\n",
    "- BsmtFin SF 1: Two outliers (>4000 sqft) with no obvious increase in price even though size is so large.\n",
    "- Total Bsmt SF: Can keep, strong upward sloping relationship. Must fix two outliers (~5000 sqft and ~6000 sqft) that did not show correspondingly large sales prices\n",
    "- 1st Flr SF: Can keep, strong upward sloping relationship. 2-3 potential outliers to fix.\n",
    "- Gr Liv Area: Good variable to keep, need to fix two outliers for area > 5000sqft. To check on correlation with 1st_Flr_SF and 2nd_Flr_SF later, if it is high, maybe we don't need the latter two variables for our regression.\n",
    "- TotRms AbvGrd: Can keep, but 3 possible outliers (13-15 rooms) with less than average prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scatter plot to look into the outliers in greater detail\n",
    "to_check = [\"lot_frontage\", \"lot_area\", \"bsmtfin_sf_1\", \n",
    "            \"total_bsmt_sf\", \"1st_flr_sf\", \"gr_liv_area\", \"totrms_abvgrd\"]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "fig.suptitle('Scatter Plot of Variables to Check')\n",
    "\n",
    "y_variable = 'saleprice'\n",
    "\n",
    "for i, axes in zip(to_check, axes.flat):\n",
    "    sns.regplot(ax=axes, data=train, \n",
    "                x=i, y=y_variable, ci=None,\n",
    "                line_kws = {'color': 'pink'},\n",
    "                scatter_kws = {'alpha': 0.5}\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose outliers to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the purpose of our model is to predict the sale price of a \"typical sale\" and not megamansion, it it worth considering removing some of the more egregious outliers seen in the scatter plots above and the filtered table of values > 5 S.D. away from mean.\n",
    "\n",
    "Propose to take out\n",
    "- Index 960: top two outliers for gr_liv_area, bsmtfin_sf_1, total_bsmt_sf, 1st_flr_sf, totrms_abvgrd\n",
    "- Index 1885: top two outliers for gr_liv_area, bsmtfin_sf_1, total_bsmt_sf, 1st_flr_sf, totrms_abvgrd\n",
    "- Index 616: 3rd highest outlier for 1st_flr_sf\n",
    "- Index 471: top two outliers for lot_area\n",
    "- Index 694: top two outliers for lot_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[[960, 1885, 616, 471, 694], to_check ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can only drop outliers from Train set, not Test set\n",
    "# Double-confirm that we chose the correct outliers to drop before\n",
    "# feeding into final dataframes\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "fig.suptitle('Scatter Plot of Variables to Check')\n",
    "\n",
    "y_variable = 'saleprice'\n",
    "\n",
    "for i, axes in zip(to_check, axes.flat):\n",
    "    sns.regplot(ax=axes, \n",
    "                data=train.drop([960, 1885, 616, 471, 694]), \n",
    "                x=i, y=y_variable, ci=None,\n",
    "                line_kws = {'color': 'pink'},\n",
    "                scatter_kws = {'alpha': 0.5}\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots look better now with the removal of the 5 outliers. There are still some outliers remaining, but not as egregious as before (they might still \"look\" like outliers but actually the scale of the x-axes are much narrower now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge numerical columns (excluding sale price as it's already in df_s) to df_train and df_test, \n",
    "# then drop the 5 outliers\n",
    "\n",
    "df_train = pd.merge(\n",
    "    left = df_train,\n",
    "    right = train[numeric[:-1]],\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ").drop([960, 1885, 616, 471, 694])\n",
    "\n",
    "# No outlier drops for test set\n",
    "df_test = pd.merge(\n",
    "    left = df_test,\n",
    "    right = test[numeric[:-1]],\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 31 columns added correctly for both sets, and 5 rows dropped for train set only\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation heatmap (and 3rd round of feature engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot correlation heatmap again, this time with better quality variables after data-cleaning and no outliers.\n",
    "I will drop dummies, but include ordinals that have been converted to numerical values ([transformation_table](#converted_ordinal_table))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of variables to includes\n",
    "to_check = ['fireplace_qu', 'garage_finish', 'garage_cond', 'garage_qual',\n",
    "           'bsmt_qual', 'bsmt_exposure', \n",
    "           'bsmtfin_type_1', 'exter_qual', \n",
    "           'heating_qc', 'kitchen_qual'] + numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap again\n",
    "\n",
    "plt.figure(figsize=(20,15)) \n",
    "\n",
    "corr = df_train[to_check].corr()\n",
    "\n",
    "sns.heatmap(\n",
    "    data = corr, \n",
    "#    annot = True,\n",
    "    linewidths = 3,\n",
    "    cmap = \"coolwarm\",\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    ")\n",
    "\n",
    "plt.title('Correlation Heatmap Plot of Numerical and Transformed Ordinal Variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low correlation with prices\n",
    "Most of the ordinals look okay in the heatmap (orange to red zone) as they have been pre-processed and selected in Section 5.1 already, but I can see some numericals that may not have predictive capability for sale prices (light blues and light oranges). Will run a filter to assess and possibly remove low correlation variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set correlation filter threshold to be 0.3 for now and see what comes up\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "abs_corr = np.abs(corr['saleprice']).sort_values(ascending = False)\n",
    "index = abs_corr[abs_corr < threshold].index\n",
    "\n",
    "index, len(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing the 18 column names tells me that we are not accidentally dropping out very important variables. To proceed with drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column drop\n",
    "df_train.drop(columns = index, inplace = True)\n",
    "df_test.drop(columns = index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-plot correlation heatmap, should be easier to read with 18 low-correlation variables removed.\n",
    "\n",
    "I can now annot heatmap without making it look too cluttered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shorten list of columns to check for multi-collinearity\n",
    "to_check = [x for x in to_check if not x in index or list(index).remove(x)]\n",
    "to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap again, dropping 18 columns names from above\n",
    "\n",
    "plt.figure(figsize=(20,15)) \n",
    "\n",
    "corr = df_train[to_check].corr()\n",
    "\n",
    "sns.heatmap(\n",
    "    data = corr, \n",
    "    annot = True,\n",
    "    linewidths = 3,\n",
    "    cmap = \"coolwarm\",\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    fmt='.1g'\n",
    ")\n",
    "\n",
    "plt.title('Correlation Heatmap Plot of Numerical and Transformed Ordinal Variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can see a number of collinear variables, e.g. fireplaces and fireplace_qu, garage_area and garage_cars.\n",
    "\n",
    "Create another heatmap to show only variables with correlation > 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap showing correlations above selected threshold\n",
    "\n",
    "plt.figure(figsize=(20,15)) \n",
    "\n",
    "# correlation threshold for filtering\n",
    "threshold = 0.7\n",
    "\n",
    "sns.heatmap(\n",
    "    data = corr, \n",
    "    annot = True,\n",
    "    linewidths = 1,\n",
    "    cmap = \"Reds\",\n",
    "    mask = corr < threshold,\n",
    "    linecolor='pink'\n",
    ")\n",
    "\n",
    "plt.title('Correlation Heatmap Plot of Highly Correlated Variables');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between two highly correlated variables, I shall drop the one that is less correlated with sale price, so we preserve as much information as possible. The numbers in parentheses are the correlations to sale price:\n",
    "\n",
    "| Variable 1 | Variable 2 | Winner (to keep) |\n",
    "|----------|---------------------|---------------------|\n",
    "| fireplace (0.47) | fireplace_qu (0.54)| fireplace_qu|\n",
    "| bsmtfin_sf_1 (0.45) | bsmtfin_type_1 (0.35) | bsmtfin_sf_1 |\n",
    "| kitchen_qual (0.69)  | exter_qual (0.72) | exter_qual |\n",
    "| 1st_flr_sf (0.65) | total_bsmt_sf (0.67) | total_bsmt_sf |\n",
    "| totrms_abvgrd (0.51) | gr_liv_area (0.72) | gr_liv_area |\n",
    "| garage_area (0.66) | garage_cars (0.65) | garage_area |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated variables\n",
    "to_drop = ['fireplaces', 'bsmtfin_type_1', 'kitchen_qual',\n",
    "          '1st_flr_sf', 'totrms_abvgrd', 'garage_cars']\n",
    "\n",
    "df_train.drop(columns = to_drop, inplace = True)\n",
    "df_test.drop(columns = to_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of final datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final structure of our datasets:\n",
    "1. Starting with 82 variables, I winnowed down to 29 variables, summarised in the Final Data Dictionary below.\n",
    "    - 17 are qualitative variables, of which 7 are ordinal, and 10 are nominal (and dummified)\n",
    "    - 12 are quantitative variables, of which 9 are continuous, and 3 are discrete\n",
    "2. Starting with 2051 rows for the Train set, I kept 2046 variables, removing 5 outliers. I did not remove any outliers from the Test set, retaining its full 878 rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__FINAL DATA DICTIONARY__\n",
    "\n",
    "| No. | Feature | DType |  Description | Remarks |\n",
    "|-----|---------|------|-------------|---------|\n",
    "| 1 | saleprice | integer | Sale price $ | \\$12,789 to  \\\\$611,657 |\n",
    "| 2 | street | integer | Type of road access to property (Paved or Gravel) | Pave = 1, Gravel = 0 | \n",
    "| 3 | neighborhood | integer | Physical locations within Ames city limits | Dummified 28 neighborhoods, dropped \"Old Town\" to become the baseline |\n",
    "| 4 | condition_1 | integer | Proximity to various conditions (\"Norm\" = Normal, \"FeederArt\" = Adjacent to arterial street or feeder street, \"Pos\" = Adjacent or near positive off-site feature, \"Rail\" = Adjacent or near North-South Railroad or East-West Railroad) | Dummified the 4 conditions, dropped \"Norm\" to become the baseline |\n",
    "| 5 | overall_qual | integer | Rates the overall material and finish of the house | 1 (very poor) to 10 (very excellent) |\n",
    "| 6 | mas_vnr_type | integer | Masonry veneer type (collapse into \"None\", \"BrkFace\", \"Stone\") | Dummified the 3 categories, dropped \"None\" to set it as baseline |\n",
    "| 7 | exter_qual | integer | Evaluates the quality of the material on the exterior | 0 (fair) to 3 (excellent) |\n",
    "| 8 | foundation | integer | Type of foundation (PConc = Poured Concrete, Others = Brick & Tile, Cinder Block, Slab, Stone, Wood | PConc = 1, Others = 0 |\n",
    "| 9 | bsmt_qual | integer | Evaluates the height of the basement | 0 (none) to 5 (excellent) |\n",
    "| 10 | bsmt_exposure | integer | Refers to walkout or garden level walls | 0 (none) to 4 (good exposure) |\n",
    "| 11 | heating_qc | integer | Heating quality and condition | 0 (poor) to 4 (excellent) |\n",
    "| 12 | central_air | integer | Central air conditioning | Yes = 1, No = 0 |\n",
    "| 13 | electrical | integer | Electrical system (SBrkr = Standard Circuit Breakers & Romex, All Others = Fuse Box over 60 AMP and all Romex wiring (Average), 60 AMP Fuse Box and mostly Romex wiring (Fair), 60 AMP Fuse Box and mostly knob & tube wiring (poor) and Mixed)| SBrkr = 1, Others = 0 |\n",
    "| 14 | functional | integer | Home functionality (Typ = Typical Functionality, All others = Minor Deductions 1, Minor Deductions 2, Moderate Deductions, Major Deductions 1, Major Deductions 2, Severely Damaged and Salvage only) | Typ = 1, Others = 0 |\n",
    "| 15 | fireplace_qu | integer | Fireplace quality | 0 (none) to 5 (excellent) | \n",
    "| 16 | garage_finish | integer | Interior finish of the garage | 0 (none) to 3 (finished) |\n",
    "| 17 | paved_drive | integer | Paved driveway (Y = paved, P = partial pavement, N = dirt/gravel | Y = 1, P and N = 0 |\n",
    "| 18 | sale_type | integer | Type of sale (\"WD\" = Warranty Deed - Conventional, \"COD\" = Court Officer Deed/Estate, \"New\" = Home just constructed and sold, \"Others\" = Warranty Deed - Cash, Warranty Deed - VA Loan, Contract 15\\% Down payment regular terms, Contract Low Down payment and low interest, Contract Low Interest, Contract Low Down, and Other | Dummified the 4 categories, dropped \"WD\" to set it as baseline dummy |\n",
    "| 19 | lot_frontage_imputed | float | Linear feet of street connected to property, with imputation using linear regression on lot area and neighborhood | 21ft to 400ft |\n",
    "| 20 | year_built | integer | Original construction date | 1872 to 2010 |\n",
    "| 21 | year_remod/add | integer | Remodel date (same as construction date if no remodeling or additions) | 1950 to 2010 |\n",
    "| 22 | mas_vnr_area | float | Masonry veneer area in square feet | 0 sqft to 1600 sqft |\n",
    "| 23 | bsmtfin_sf_1 | float | Basement finished area in square feet | 0 sqft to 5644 sqft |\n",
    "| 24 | total_bsmt_sf | float | Total square feet of basement area | 0 sqft to 6110 sqft |\n",
    "| 25 | gr_liv_area | integer | Above grade (ground) living area square feet | 334 sqft to 5642 sqft |\n",
    "| 26 | full_bath | integer | Full bathrooms above grade | 0 to 4 |\n",
    "| 27 | garage_area | float | Size of garage in square feet | 0 sqft to 1418 sqft |\n",
    "| 28 | wood_deck_sf | integer | Wood deck area in square feet | 0 sqft to 1424 sqft |\n",
    "| 29 | open_porch_sf | integer | Open porch area in square feet | 0 sqft to 547 sqft |\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final dataframes to .csv files\n",
    "df_train.to_csv('../datasets/df_train.csv', index = False)\n",
    "df_test.to_csv('../datasets/df_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I have to transfer the remaining code (model selection et al) to another file as my Jupyter notebook would sometimes stop running and skip certain chunks of code if all the code is here... Pls go to ethan-project-2code-part2.ipynb to continue your marking, thanks!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
